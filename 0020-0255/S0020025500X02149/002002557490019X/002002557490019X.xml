<doc:document xmlns:doc="http://www.elsevier.com/xml/document/schema"><rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"><rdf:Description rdf:about="http://dx.doi.org/10.1016/0020-0255(74)90019-X"><dc:format xmlns:dc="http://purl.org/dc/elements/1.1/">application/xml</dc:format><dc:title xmlns:dc="http://purl.org/dc/elements/1.1/">Parameter estimation using splines</dc:title><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/"><rdf:Seq><rdf:li>Demetrios G. Lainiotis</rdf:li><rdf:li>J.G. Deshpande</rdf:li></rdf:Seq></dc:creator><dc:description xmlns:dc="http://purl.org/dc/elements/1.1/">Information Sciences 7 (1974) 291-315. doi:10.1016/0020-0255(74)90019-X</dc:description><prism:aggregationType xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">journal</prism:aggregationType><prism:publicationName xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">Information Sciences</prism:publicationName><prism:copyright xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">Copyright © unknown. Published by Elsevier Inc.</prism:copyright><dc:publisher xmlns:dc="http://purl.org/dc/elements/1.1/">Elsevier Inc.</dc:publisher><prism:issn xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">0020-0255</prism:issn><prism:volume xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">7</prism:volume><prism:coverDisplayDate xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">1974</prism:coverDisplayDate><prism:coverDate xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">1974</prism:coverDate><prism:pageRange xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">291-315</prism:pageRange><prism:startingPage xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">291</prism:startingPage><prism:endingPage xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">315</prism:endingPage><prism:doi xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">10.1016/0020-0255(74)90019-X</prism:doi><prism:url xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">http://dx.doi.org/10.1016/0020-0255(74)90019-X</prism:url><dc:identifier xmlns:dc="http://purl.org/dc/elements/1.1/">doi:10.1016/0020-0255(74)90019-X</dc:identifier></rdf:Description></rdf:RDF><dp:document-properties xmlns:dp="http://www.elsevier.com/xml/common/doc-properties/schema"><dp:raw-text>INFORMATION SCIENCES 7, 291-315 (1974) 291

Parameter Estimation Using Splines*

DEMETRIOS G. LAINIOTIS

Department of Electrical Engineering, State University o f New York,
Buffalo, New York 14214

and

J. G. DESHPANDE
Department of Electrical Engineering, Indian Institute of Technology, New Delhi, India

ABSTRACT

In this paper the estimation of unknown, time-invariant parameters that, if known, com-
pletely specify a discrete, linear dynamic model with Gaussian disturbances, is considered.
Following the Bayesian approach the unknown parameters are modeled as random variables
with known a priori probability density. Optimal, in the mean-square-error sense, estimates
are desired. However, this requires recursive updating and storage of a non-Gaussian, and
more importantly, a nonreproducing density. Therefore, exact realization of the nonlinear
parameter estimators requires immense computational effort and storage capacity. To alle-
viate these difficulties, splines functions are used for the approximate realization of the
Bayesian parameter estimation algorithm. Specifically, variation-diminishing splines are
used, which are pieces of smoothly tied polynomials to approximate the a posteriori proba-
bility density (pdf). This approximation of the pdf is specified in terms of a finite number
of parameters, yielding a readily implementable approximation of the exact but unimple-
mentable parameter estimation algorithm. A convergence theorem is obtained for the con-
vergence of the spline algorithm to the optimal algorithm, as well as two implementable

continued

1. INTRODUCTION

In this paper the estimation of unknown parameters that, if known, com-
pletely specify a discrete, linear, dynamic system with Gaussian disturbances is
considered. Optimal in the mean-square-error-sense Bayesian estimates are de-
sired. As is well known [1-6,14], to obtain these estimates it is necessary to
perform recursive updating and intermediate storage of the a posteriori probabil-
ity. However, as is also well known [1-6, 14], the a posteriori probability is
non-Gaussian, and the associated estimation problem both for the parameters
and the state, is a nonlinear one. Therefore, "exact" realization of the nonlinear

*This work was supported by the Air Force Office of Scientific Research under Grant
AFOSR 69-1764A and Joint Services Electronics Programme Grant AFOSR 69-1792.

© American Eisevier Publishing Company, Inc., 1974

292                       DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE

error bounds. Extensive numerical simulation indicates that the spline algorithm performs
well, yielding parameter estimates very close to the true values. In summary, the salient
characteristics of the proposed spline-based estimator are as follows: (a) it is readily imple-
mentable, its realization requiring essentially a bank of Kalman filters and integration of
piece-wise polynomial functions; (b) the spline algorithm is based essentially on an im-
bedded quantization algorithm, namely, the one corresponding to the bank of Kalman fil-
ters used; (c) as such, the spline estimator has greater computational requirements than the
associated quantization algorithm; but (d) the spline algorithm is more accurate than the
quantization algorithm. In view of the above, the spline-based estimator should prove useful
in practical applications of parameter estimation as well as of general nonlinear estimation.

estimators is nonfeasible since exact realization requires immense computational
effort and storage requirements [1-7,14].

Several approaches [2, 14] have been taken to alleviate these difficulties, re-
sulting in approximate but realizable nonlinear estimators of varying complexity
and efficacy. Most notable of these approaches are the so-called "global" ap-
proaches [2, 14], such as the quantization approach [1-7,14-16], the Gaussian-
sum approach [2, 6, 16-21], the spline function approach [10, 13], and others
[2,14].

The spline function approach, first proposed by de Figuerido [10] for general
nonlinear estimation problems, has several important advantages. Namely, its
use yields approximate nonlinear estimations that are relatively easy to imple-
ment, and have excellent computational and performance characteristics. In
this paper, splines are used to obtain approximate but attractive parameter esti-
mation algorithms for the problem indicated above. Specifically, splines are
used to obtain parameter identification algorithms. Following the Bayesian
approach the unknown parameters are modeled as random variables taking val-
ues in a fixed unknown parameter space with a given a priori probability. The
discrete version ofLainiotis' "partition" theorem [4, 6, 7] is used to obtain a
recursive relation for the a posteriori probability of the unknown parameters.
As stated previously, the a posteriori density is non-Gaussian and more impor-
tantly it is not reproducing. As such it can not be specified by means of a finite
number of values, unless the unknown parameter vector takes values from a
finite set. However, in most cases the unknown parameter vector takes on a
continuum of values, and as such the identification algorithm can not be imple-
mented exactly. Hence some kind of approximation has to be made for its
implementation. We use the variation-diminishing spline approximation, intro-
duced by Schoenberg [8], to approximate the a posteriori probability (pdf).
Since, the spline functions used are pieces of polynomials tied together
smoothly, this approximation of the pdf can be specified by means of a finite
number of parameters, yielding an implementable parameter estimation
algorithm.

In the next section, the parameter estimation problem is formulated and the

PARAMETER ESTIMATION USING SPLINES                             293

associated exact Bayesian recursive estimation algorithm is obtained. In Sec. 3,
the mathematical theory of the relevant spline functions is briefly reviewed as a
background, and in Sec. 4, the spline-based parameter estimation algorithm is
given. Section 5 pertains to the performance of the spline estimator. Specif-
ically, the convergence of the spline algorithm to the exact, optimal, Bayesian
identification procedure is investigated, and two error-bounds are derived. In
Sec. 6, several numerical examples are given in which the spline algorithm is
utilized.

2. PROBLEM FORMULATION AND BAYES RECURSION

Consider a linear, discrete time dynamic system described by the following
equations:

x(k+ï)=&lt;ï&gt;(k+Ï,k;6)x(k)+w(k),             (l)
z(k)=H(k,6)x(k)+v(k),

A-0,1,2,...,                          (2)
where,

x(k) -n dimensional state vector,
w(k)n dimensional input noise vector,
z (k)-m dimensional observation vector, and
u(k)m dimensional observation noise vector.

The state transition matrix &lt;î&gt; and the observation matrix H have appropriate
dimensions. 6 is a p-dimensional vector of unknown parameters which are to be
identified. The initial condition x(0) is a Gaussian random variable with mean
J?(0/0, 6) and covariance P(0/0, 0).

{w(^)} and {v(k)} are mutually independent zero mean, white, Gaussian
random sequences:

E{w(k)wT(f)}=Q(k,9)6|,                 (3)
WÄO^a)}^^,^.                 (4)

The parameter vector 9 is assumed to be time-invariant, and is modeled as a
random variable taking values in fixed parameter space 0 with known a priori
probability density function (pdf)p(0).

Define the observation sequence

Àfcê{z(0),...,z(^)}.

Our goal is to identify the parameter vector 6. We adopt the Bayesian ap-
proach to the identification problem, and define the parameter estimate as
follows.

294                       DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE

Definition. The parameter estimate of the parameter vector 6 at time k,
denoted by 0 (k) is given by

^)= f o pk (0) dO,                      (5)
&gt;'©

where, pfc(0) is the probability density function of 9 conditioned on the
observations \ic.

Hence, to identify the parameters we need the a posteriori pdf of 9. The dis-
crete version of Lainiotis partition theorem [4, 6-7] gives this pdf. To write the
expression ÎOT p)c(9) we need the concept of model conditional state estimate.

Definition. The model conditional (or 0 -conditioned) state estimate at time
k, denoted by $(k/k, 9), is given by

S(k/k,9)=E[x(k)l\k,6}                   (6)

for each 9 £ ©.

Since conditioned on 9 and Xfc,^k is Gaussian,j?(fc/fc, 6) is given by the Kal-
man filter equations assuming that the model (1-4) takes that particular value
of 9. Thus, S(k/k, 9) is given by the following Kalman filter equations.
For k = 1, 2,..., and for every 9  ©

S(k/k, 9) = x(k/k - 1, 9) + F(k, 9) z(k/k -1,9),                   (7)
x(k/k- l,9)=&lt;î&gt;(k,k- 1,9) x(k- l/k- 1,9),                    (8)
j?(0/0,9) is given,

z(k/k - l, 9) = z (k) - H(k, 9) x(k/k -1,0),                        (9)
F(k, 9) = P(k/k -1,9) H7'(k, 9) [H(k, 9) P(k/k -1,9)

HT(k,9)+R(k,9)}~l,                          (10)
P(k/k- l,9)=&lt;î&gt;(k,k- l,9)P(k- ï/k- Ï,9)î&gt;T(k,k- 1,9)

+Q(k-l,6),                                   (11)
P(k/k, 9)= [I- F(k, 9) H(k, 9)] P(k/k -1,9),                   (12)
P(0/0, 9) is given.

Using Eqs. (7-12), the a posteriori pdfpic(9) is given by [4, 6]

^._    Wp^(9)                        ^

Z-k(0)pfc-i (9)d9
"ô

PARAMETER ESTIMATION USING SPLINES                             295

where

LkW = I PzWk - 1,6) I -^ exp (4 II z(^ - 1, Q) || ^.(^-i,(,)},  (14)
P,(k/k - 1, 0) ê [//(A:, 0) TW - l, 0) //7'^, 0) + R (k, 9)],     (15)

and I  l denotes the determinant of the matrix.

Note that (13) is a recursive form for calculating the a posteriori pdf. Sims
and Lainiotis [5] were the first to give this recursive form. It is clear from Eqs.
(13-15) that pie (0) is, in general not Gaussian, even if, the a priori distribution
of 6 is Gaussian. In fact, p/c (6) can not be specified by means of a finite number
of arbitrary constants. This fact can be explained as follows. If, the parameter
space 0 is infinite, we need to solve an infinity of Kalman filter equations
(7-12). Each Kalman filter requires the storage of two quantities, namely, the
model conditional mean ïc(k/k, 6) and the covariance P(k/k, 6). Since pic (9) is
infinite dimensional, it is not possible to calculate pic(6) exactly. Therefore,
some kind of approximation has to be made.

Lainiotis in [1-3,4, 6] has suggested the quantization technique. In his
technique the parameter space is quantized to a finite number of quantization
levels. Then the a priori pdf p(6) is a train of delta functions:

m

p(6)=^p',6(6-6,)
1=1

Using this, the a posteriori pdfpic(9) can be calculated recursively as

m
P(.ô)=^pk6(6-6,),

i=l

, _ Lk(ßj)pk-i

P^ n,

Z L^,)P{-
/=l

and the corresponding mse estimate is given by

..                      m

6 {k} =^ P, 9,.

It is apparent that the parameter estimation accuracy will depend integrally
on how fine the quantization is. Namely, the finer the quantization the better
the performance of the identification algorithm. However, the finer the quanti-
zation, the larger the computational and storage requirements will be. To alle-
viate these difficulties several on-line or adaptive quantization schemes have
been proposed such as those of Sengbush and Lainiotis [15] and Bucy and
Senne [16].

296                       DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE

However, it must be noted that however fine the quantization may be, the
resulting approximation is sparse over the unknown parameter space. To allevi-
ate this and to obtain smooth approximations over the parameter space, splines
will be used to approximate the pdf. It will be seen in the following sections,
that the resulting spline-based approximately Bayesian parameter estimation
algorithms are easily implemented, have greater accuracy than the corresponding
quantization-based algorithms and necessitate for their realization a bank of
Kalman filters just as the quantization algorithms do.

Prior to presenting the spline-based parameter estimation algorithm, the
mathematical theory of the relevant spline functions is briefly reviewed in the
next section.

3. VARIATION DIMINISHING SPLINE APPROXIMATION OF A FUNCTION

In this section we shall briefly review the mathematical theory of variation
diminishing spline functions. First we will consider functions of one variable,
and then extend it to functions of several variables.

3.1. SPLINE APPR OXIMA TION OF FUNCTIONS OF ONE VARIABLE

Most of the following material for approximating the functions of one vari-
able is taken from Marsden [9]. We start with the definition of a polynomial
spline function.

Definition. A polynomial spline function of degree m and knots
11, t-i,..., t K is a function s e C"1"1 (-°°, °0) such that s is a polynomial of de-
gree m, in each of the open intervals (-°°, ti), (t\, t^ ),..., (t^-i, t^), (t^, °°).

Denote the class of polynomial splines of degree m and knots ti, t-s.,..., t^
by S"1 (ti,..., tJf} or, when the context is clear, by S. It can be shown that S
is a real vector space of dimension m + K + l. Curry and Schoenberg [8] have
introduced B-splines which are used to compute local bases for S. For our pur-
pose we need only the B-splines which we define, presently.

Let m be a positive integer and {/,}Jloo be a doubly infinite sequence of real
numbers satisfying

Define

where

ti&lt;t,^&lt;t,^t.                      (16)
m(t;x)ù(m+l)(x-t)m,

{(x-tr t&lt;x,

Oc-O'"-

l O       t&gt;x.

PARAMETER ESTIMATION USING SPLINES                            297

Then tlie ;th B-spline denoted by M, is given by

^(i)(,-,g

      n  '&gt;
/=o      ^'tVi-n)

where £),'(fy+,) is derivative of £);(?) evaluated at f = /y+,, and

D(t)=(t-t,)(t-t,^...(t-t,^^.            (18)

TVotó. Actually B-spline is defined as (m + l)th divided by the difference of
m(t;x) with respect to the points x ={{,..., t,+rn-n  (See [7] for the definition
of divided difference.) For distinct knots, i.e., when t, ^ ?,+i, (17) is a good
formula to calculate the B-splines. For the case of equal knots, (17) fails, and
one has to resort to the divided difference. For details we refer to [10, 13].

Some of the important properties of the B-spline are

(a) ^.e^O,,...,?,^),

(b) Mi(t) &gt; 0 with equality if and only if t &lt; t,, t &gt; ?i+,+i,

(c) f M,(0^=l,

J-oo

(d) the kih derivative ofM,(t) has exactly k simple zeroes on (/,, ?,.,.,+;),
k=0, ï,...,m- l.

Now we are ready to give an approximation of a function defined on a closed
and bounded interval [a, b] of the real line. Define the mesh

^:a=to&lt;ti &lt; . . .&lt;tK=b.

t,:i= 0,... K are called knots. Let m &gt; 2 be an integer. Extend A to Ai
as follows

Ai : a = ?-, =... = to &lt;... &lt; IK = .. . t^m = b-
For ; = - m,... , K - 1, define

N^t)^1^^1 Mi(t\                     (19)

^ù1'^^"                   (20)
m

The functions ./V; (t) are called fundamental functions and i;, are called nodes.
Since ./V, is nothing but a normalized B-spline, Ni  5"" (t,,.. ., t^m +1 ) Two

298                        DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE
important properties of the fundamental functions are given by

'EM(0=i,                  (2l)

i=m
and

'Swt)=t                (22)

i=m
for every t  [a, b], provided

N-m(a) is replaced by N-m(a'').

Definition. For any real-valued function/with domain [a, b] and for fixed
m &gt; 2 and fixed A, the spline approximation off, denoted by S'^  is given by

S^m^f^Nid)                   (23)
i=m

a&lt;t&lt;b.

Since for a given w and A, there is only one possible spline approximation of
a function, whenever it is clear from the context, we will denote SS(f) by Sf.
Some of the important properties of the spline approximation are

(a) S^fGC"1'1 (in particular S f is continuous on [a,b]);

(b) S f (a) = f (a) and Sf(b) =(&amp;);

(c) S f is exact if/is linear, i.e., if/(Q = at + ß then Sf(t) = at + (3;

(d) the spline approximation is variation diminishing, i.e., besides following
(c), S [f (t) - l (t)] has no more changes in signs on [a, b], that [f (t) - l(t)]
has, where l(t) is any linear function on [a, b].

The variation diminishing property [8] is very important. It tells us how well
Sf(t) follows f(t). For that reason the approximation (23) is sometimes called a
variation-diminishing spline approximation.

If a continuous function is approximated on [a, b} by using (23), it is pos-
sible to get error bounds on approximation, and to prove the convergence of the
spline approximation. Hence, the spline approximation of Eq. (23) is best suited
for approximating any continuous function. We give the actual error bounds
and the convergence theorem in [13] but note the most desirable property of
the spline approximation implied therein, namely, the larger the degree m of the
spline approximation and/or larger the number of knots in the mesh A, the bet-
ter the spline approximation of continuous functions. This is the most antici-
pated and the most desired property of this approximation.

PARAMETER ESTIMATION USING SPLINES                             299

3.2. SPLINE APPROXIMATION OF FUNCTIONS OF p VARIABLES

In the following we shall follow the approach of Munteanu and Shumaker
[12] (also see DeFigueiredo and Jan [10] ). We construct methods leading to
tensor product splines.

We are interested in approximating a function/defined on a compact inter-
val © C/?P into R, where

@=X[ab,].                         (24)

A typical element ? of © will be written as (t1,.. ., t?). Define for
 = 1,.. ., p, meshes

^i:a,=ti&lt;...&lt;t'^=b,.

Let Wi,.. ., Wp &gt; 1 be integers. Then m each direction; = 1,.. ., p define the
fundamental functions^ and the nodes ^ using Eqs. (19-20); -Wy &lt; ; &lt; K.J.
Then the spline approximation of any function/is defined as

z^//(rl,...,^)ê t ... i /(H ...^)

/ 1                                  ''1s-"1!        lp="lp

N\(tl)...NP^tp). (25)
Whenever {Ay} and {m^} are clear from the context, we shall denote

X S^'f(tl,...,tP)by X Sf(t).

=!   /                  /=1

The above notations can be simplified if we define an index set /, generalized
fundamental functions 51, and generalized nodes -a ; £/ as follows:

An element i belongs to index set /, if

; = (;i,.. . , /p),

where ;, is an integer such that - m, &lt; i j &lt; Kj, for; = 1, 2,. . . , p.
Define for each ; £ I, the generalized fundamental function A^- as

î^i(t)ÙN]^tl)...N^(tp),                  (26)
and the generalized node z, as

»,=a;,,...,^).                 (27)

Using these definitions, Eq. (25) can be rewritten as

ÏSf(t)= Z/(^,(0.                   (28)
/+l       ;e/

Note the similarity between Eqs. (23) and (28).

300                     DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE
4. THE SPLINE ALGORITHM

The most formidable task in implementing the identification scheme given m
Section 2 is the calculation of the a posteriori pdf of 6. Unless the parameter
space Q is finite, it is necessary to run an infinite number of Kalman filters, one
corresponding to each 0 £ 0. We use the variation diminishing spline approxi-
mation of the last section to approximate the pdfpic(6).

Assumption. The unknown parameter 9 is bounded, i.e., there exist real
numbers a, and b, such that for/ = 1,2,..., p, -°° &lt; a, &lt; Q' &lt; b, &lt; u0.

This assumption means that © is nothing but a compact interval ofR1'.
Hence, using Eq. (13), the assumption can be shown to be equivalent to vanish-
ing ofp(0) outside a compact interval ofR1'.

This assumption leads to the fact that at any time k, the estimate of the pa-
rameter ^(k) £ ©. Hence, before the identification starts, one should have a
fairly good idea about this compact interval in which the true value of the pa-
rameter vector lies. But, this is not too much to ask, since one usually has a
good idea about bounds on the values of the unknown parameters.

THE SPLINE ALGORITHM
Initialization

(a) Select {m,} and {A/},/ = 1,... ,p.

(b) Construct the index set /, and calculate the generalized fundamental func-
tions ÎI; and the generalized nodes », for i £ /.

(c) Using Eq. (28), approximate the a priori probability density function
Po(8). If, we denote this approximation by (po(0)), then

&lt;Po(0)&gt;=Zpo(»,)^,-(0), 0e©,              (29)
i(=/

where po(z{) is the valve of the function po at the node z,.

Iterations-k =1,2,...

(a) Calculate S(k/k, 9) and P(k/k, 6) when 6 takes values at the nodes. [Use
eqs.(7-12).]

(b) For / e , calculate Z-fc(»,) using Eq. (14, 15).

(c) Read the values ofpic-i (î,) from the available approximation &lt;pfc_i&gt; of
Pk-i. Denote these values by (pk-i (Ci)). For each / £/, multiply Z-k(ï;) by
(pfc-i (s-i)). Using these values, approximate the product function
[Lic (0)&lt;pA;-i ((?)&gt;]. If&lt;Lfe(0)pfc_i(0)&gt; denotes this approximation, then from
Eq. (28),

&lt;^(0)Pfc-iOO&gt; = Z W»,)&lt;Pfc-i(»,)&gt;W.                (30)
«=

PARAMETER ESTIMATION USING SPLINES                              301

(d) Integrate (Z, fe(0)pfc_ i (0)) over ©. Denote this integral by D. Then,

D= ( (Lk(6)pk-t(6))de.                  (31)

-Q

(e) The approximate a posteriori pdf is then obtained by dividing Eq. (30) by
(31). If we denote this approximation by p^ (9), then

^.a.(^-,(.)&gt;            p

(f) Find the parameter estimate O (k) as follows:

^)= ( 0 &lt;Pfc(0))û?0.                  (33)
-e

(g) Select new {m,} and {Ay},/ = l, 2,. .. ,p.

(h) Construct new index set /, and calculate new generalized fundamental
functions Tl,, and new nodes ?; for ; £ /.

(i) Using the values of$(k/k, 6) and P(k/k, 6) at the old nodes, approximate
these functions using Eq. (28), with the old fundamental functions. From these
approximate functions, read the values ofS(k/k, 0) and P(k/k, 0) when 0 takes
the values at the new nodes.

(j) Go back to step (a).

The following remarks on the spline algorithm are pertinent.

Remarks, (a) The selection of spline degrees {m,} and meshes {Ay} is very
important. It is desirable to have a large value for m j and to have as many knots
in the meshes as possible, in order to get better approximation. However, if
{K.,}, and {m,} are large numbers we have a large number of nodes î and con-
sequently, a large number of Kalman filters to run. Thus we would like to keep
the values of {Kj} and {w/} small. While deciding the m j and Ay one has to
take these conflicting facts into account.

(b) The memory requirements for the generalized nodes and the generalized
fundamental functions are as follows. In direction , there are (Kj + rrij) nodes
and (K, + w/) fundamental functions of one dimension. Each such fundamental
function is a polynomial of degree m, on each of the K, intervals of the mesh in
that direction. To store each w.-degree polynomial (wy +1) coefficients are
needed. Hence in the/th direction the total memory required for the nodes and
the fundamental functions is (Kj + wy) [(m/ + l)2 + 1].

(c) Note that &lt;) does not necessarily denote the spline approximation [given
by Eq. (28)] of the exact function . For example, (L^pk-i'&gt;, given by Eq. (30),
is not the spline approximation L^p^.t. Instead, it is the spline approximation

302                       DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE

of Lic (pk-i&gt;. Similarly (pp is not the spline approximation of the exact func-
tion p^.

(d) The integrations involved in Eqs. (31) and (33) are easy to carry out.
They are merely a sequence of integrations of the polynomials. Hence, these
integrations are straightforward.

(e) Usually, it is not necessary to change {wy} and {Ay} at each time step k,
as required by step (7) of the algorithm. Very seldom the {wy} are changed.
Most of the time Wy is the same for ally. A method for changing { Ay} is given
in [13]. This method is based on intuition and common sense. Note that,
since S(k/k, 9) is an n-dimensional vector and P(k/k, 6) is an n X n symmetric
matrix, there are respectively n and n(n + 1)/2 approximations needed in step (i)
of the algorithm.

5. PERFORMANCE OF THE SPLINE ALGORITHM

In this section, we deal with the convergence of the spline algorithm of the
last section to the exact identification scheme of Sec. 2. In particular, one con-
vergence theorem is given, and two error bounds are derived. This theorem and
the error bounds depend heavily on the convergence theorem and bounds given
in [13]. They are given for approximating continuous functions only. Hence,
we assume that all the functions being approximated in the spline algorithm are
continuous on ©. This assumption is not restrictive. Since, if^(k + Ï, k, 9),
H(k, 9), Q(k, 6),R(k, 6),P(OlO, e),S(0/0,6), andp(0) are continuous on Q,
then it can be shown that [ 13], pfc(0), Lic (6), x(k/k, 6) and P(k/k, 6) are indeed
continuous on Q. The system parameters 4), H, Q, R, P(0i0, J? (0/0), and p (0)
are almost always continuous on 0. Therefore, we do not lose much by assum-
ing that pfc, Lic, x(k/k, 9) and P(k/k, 6) aie continuous on ©.

We first state the convergence theorem.

THEOREM 1. A necessary and sufficient condition that the approximation
&lt;Pk(0)&gt; given by the spline algorithm [Eq. (32)] converge to pfc(0) uniformly in
Q, for all k,is that

,. 11^-11
lim=0   /=l,2,...,p
m,

where || Ay || = max (t\ - ^_i ). [Note that the convergence in this theorem

means the convergence of the sequence of functions &lt;.pk(6)) as m, and Ay are
changed.]

Proof. The proof is a direct consequence of the convergence theorem given
in [13].

This theorem is a pleasant surprise, since it says that the spline algorithm can

PARAMETER ESTIMATION USING SPLINES                             303

be made to work as close to the exact scheme as desired by either using higher
degree polynomials or by taking more knots in the mesh. However, once values
of {m,} and { Ay} are fixed, we would like to know how well we are doing in
identifying the parameters. The following two error bounds give an idea about
that. But, first, let us make our notations clear.

(pk-i(0)) -Available approximation of the a posteriori pdffrom the previous
step k - 1.

&lt;Z,fc(0)pfc_i(ö)&gt;-Spline approximation of [Liç(9)(pic_i(6))] obtained from
Eq. (30).

&lt;pfe(0)&gt; -Approximation ofpic(9) obtained in the spline algorithm using
Eq. (32).

Define,

-I

D^   L,(e)p,^(6)d0,                 (34)

"O

&lt;£»fc&gt;= f {Lk(6)p^,(6)d6.                 (35)
-o

Assume that at step k, the following two bounds are available: for 6 e Q

\(L^6)}-Lk(ß)\&lt;pk                   (36)

l^fc(0)Pfc-&gt;W&gt;-^(0)&lt;Pfc-i(0)&gt;l&lt;§fe            (37)

where (L^(Q)) is the spline approximation ofZ,^(0). These two bounds are the
bounds on the error in approximation, and, hence, can be found by employing
the techniques of Appendix A.

If g is any function defined on ©, then its L"'-norm and L1 -norm are defined
respectively as:

II^IL= max \g(6)\,

0 £ ©

IIS'lli = | \g(6)d6.

'B

The following two propositions, which are proved in [10], give two error
bounds on approximatingpfc(0).

PROPOSITION 1. If,

then

IKpfc-i&gt; - Pk-i L^fc-i,

ll&lt;Pfc&gt;-PfclL&lt;efc, (38)

304                       DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE

where

efc=,n, 1   liafc&gt;IL+Pfcefc-i+5fc+^ll&lt;Pfc&gt;ILo.       (39)
&lt;ü&gt; - bo

p             r

&amp;D=(5fc+efc-iPJOn (&amp;/-a/)+et-i   (L(0))d6.       (40)
/=i             -a

PROPOSITION 2. /,

ll&lt;Pk-l&gt;-Pfe-llll&lt;T?fc-i

ll&lt;Pfc&gt;-Pfclll&lt;T?k,                           (41)

7?k =  II &lt;^&gt; II«, + Pfcîîfc-i + 5 n (b, - a,)- (42)

&lt;Z)&gt;

/=i

Using these two propositions, the following two theorems can be proved very
easily. In these two theorems

§\k)= f 9pk(6)d6,                  (43)

'ia

and

^)^ 0(pk(0)&gt;do,                    (44)
'©

^f(k) and ^-(fc) denote the/th elements of these two vectors.
THEOREM 2. For j = 1,...,p

I ^(k) - g/W | &lt; T?fc max [ I a/. |, | b, \ ].              (45)
THEOREM 3. Forj-= 1,.. . ,p

I ^,(k) - ^(k) | &lt; T?fc max [ | a, |, | &amp;/1 ].                (46)

Note that, both the error bounds on pic (6) given by Eqs. *38-40) and (41-42)
are recursive, and they can be computed on line. These bounds increase as k in-
creases, since the compensating effect of the successive approximations, if any,
is not taken into account while obtaining these bounds. Nevertheless, they give
a good idea about errors in approximations.

PARAMETER ESTIMATION USING SPLINES                             305

6. NUMERICAL EXAMPLES

Example 1. Consider a scalar system described by the following equations

x(k+ \)=^&gt;x(k)+w(k),
z(k)=x(k)+u(k).

The random processes {w(k)} and {v(k)} are assumed to be zero-mean,
Gaussian, stationary, and white with the following statistics:

w(k)~ ^{0.0,0.2} for all k,
ü(fe)~ ./V {0.0, 0.1} for all k.

The initial condition x(0) is a random variable with the statistics:

x(0)~ N {1.0,0.5}.

We assume that all the parameters of the system are known except for the pa-
rameter ip. Hence, 6 = ^. The data is generated with ^ = 0.65. Thus the true
value e*= 0.65.

It is assumed that ip, as a random variable, can take values in the closed
interval [0.0, 1.0]. It is also assumed that &lt;p is uniformly distributed on [0.0,
1.0].

Cubic splines (i.e., m = 3) are used for the spline algorithm. Since po(f) is
uniform, we place the knots at equal distance within [0.0, 1.0] at time 0.
The spline algorithm is used for 500 steps of time with various number of knots.
The decision about changing the mesh was made according to the method given
m [13].

The spline algorithm was used for several noise samples, out of which, the re-
sults for one sample run are presented here. Figure 1 (a-f) shows the a posteriori
pdf at time k = 500 for the case ofA'= 2, 3, 4,6, 8, and 10. We expect that,
since the true pdfpfe(ip) converges to a ô function as k increases, and since 500 is
quite a large time, the true pdf should have a large peak near iß = 0.65. We also
know that the larger the number of knots, the better is the approximation. This
is seen clearly from Fig. 1. As the number of knots is increased, the pdf at k =
500 increases sharply around ^ = 0.65 and falls to zero away from this true
value. It is comforting to note that the pdf looks almost the same (as far as iden-
tification is concerned) for K = 6, 8, or 10. It shows that we do not have to in-
crease the number of knots indefinitely for better and better identification.

To look into these results more closely, we consider two of the above-
mentioned six cases in more detail. These are the cases ofA'=6 and 8. The

DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE

K ° 2




10

K °3










10

K ° 10

0.5

(f)

1.0

Fig. 1. Probability density function of 6 foi different number of knots in the
mesh.p5oo(8), Po(9)&gt; Example 1.

PARAMETER ESTIMATION USING SPLINES                             307

initial meshes at time k = 0, for these two cases are
K=6, k=0

to       ti       t-t       ty,       f4       '5       t6

0.000 0.167 0.333 0.500 0.667 0.833 1.000

À'=8, Ä;=O

to     ti     tî     73     ?4     ts     to     h     ts
0.000 0.125 0.250 0.375 0.500 0.625 0.750 0.875 1.000

In Fig. 2, the evolution of the a posteriori pdfpfc(0) is shown by plotting these
functions for some representative values of time k.

The spline algorithm is compared with the quantization scheme given in
Sec. 2 on the basis of equal number of Kalman filters. There are K + 3 Kalman
filters to be run in the spline algorithm. The quantized values are taken at the
nodes of the mesh corresponding to the spline algorithm.

In Fig. 3 and 4, the parameter estimates are plotted as functions of time for
K = 6 and 8, respectively. The solid lines in these figures correspond to the
spline algorithm and the broken line to the quantization scheme. [These curves
are plotted on logarithmic scale and the curves between k = 450 to 500 are mag-
nified on the linear scales.]

Both these figures show that, given sufficient time, the spline algorithm does
identify the true value of the parameter. At the same time both these figures
show that the spline algorithm does the job of identification better than the
quantization scheme, as we can see that as time increases the curve for spline
algorithm stays closer to the true value of the parameter than the curve for
quantization scheme.

Example 2. For second example, we again consider a first order system in
which a different parameternamely, the measurement "matrix," is identified.
In particular, the following system is considered:

x(k+ï)=OSx(k)+w(k),
z(k)=-hx(k)+v(k).

The statistics of different random variables are as follows:

x(0)~N {0.1, 1.0},
w(k) ~N {0.0, 0.25}, for all A:,
u(k)~ N {0.0,0.1}, for all k,

DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE




st
o




PARAMETER ESTIMATION USING SPLINES




DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE




S

PARAMETER ESTIMATION USING SPLINES




«l h

3 a

h fc

DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE




&lt;M

w
'S.

PARAMETER ESTIMATION USING SPLINES                              313

The unknown parameter is taken to be A. The data was generated using
h = O.S. Hence, h* = 0.5. The random variable h was assumed to be uniformly
distributed on the closed interval [0.1, 1.0] at time 0.

[This particular problem is like having a potentiometer which gives the output
of the plant as a fraction of the state x(k). The nature sets its value at 0.5, but
all we know is that the potentiometer is set anywhere between 0.1 and 1.0, and
we have no other knowledge as to where the setting might be.]

The spline algorithm was used to identify h with value of m set at 3. The
mesh was taken with K = 6 and the knots were spaced uniformly. The initial
mesh is

k=0

to      tl      t-t      ty,      /4      ts      t(,

0.10 0.25 0.40 0.55 0.70 0.85 1.00

The a posteriori pdîpic (h) is plotted for some typical values of k in Fig. 5.
Once again, we see that p^ (h) approaches a 6 function near the true value of
0.5 as k increases. In Fig. 6, we plot the estimate of/zasa function of time. It
is seen that h (k) goes near to 0.52 when the spline algorithm is used; whereas,
with the quantization scheme, it converges to 0.55, which is the quantized
value nearest to the true value h* = 0.5.

7. CONCLUSIONS

The problem of parameter identification of unknown linear stochastic sys-
tems, excited by white Gaussian noise and on which linear observations are
made in the presence of additive white Gaussian noise is considered. The un-
known parameters are modeled as random variables taking values in a fixed pa-
rameter space with known a priori probability density function (pdf). The
parameter estimate is defined as the conditional mean of these random variables
conditioned on the observations made on the system. However, this approach
leads to nonparametric a posteriori pdf for the unknown parameters. Variation
Diminishing Spline Functions, which are smoothly tied pieces of polynomials,
are used to approximate these a posteriori pdfs. In particular, a spline algorithm
is obtained which approximately implements an otherwise unimplementable
parameter identification scheme. A convergence theorem is obtained for the
convergence of the spline algorithm to the exact identification scheme. Two im-
plementable error bounds are obtained on the errors in using the spline algo-
rithm. Two numerical simulation examples are presented which show that,
given enough time, the spline algorithm gives a parameter estimate which is very
close to its true value.

In conclusion, we summarize the advantages and the characteristics of the

314                       DEMETRIOS G. LAINIOTIS AND J. G. DESHPANDE

spline-based algorithm. Specifically, (a) the algorithm is easily implemented
since its realization requires essentially a bank of Kalman filters and integration
of piece-wise polynomial functions; (b) the spline algorithm is based essentially
on an imbedded quantization algorithm, namely, the one corresponding to the
bank of Kalman filters used; (c) as such, the spline algorithm has greater com-
putational requirements than the associated quantization algorithm; but (d) the
spline algorithm has better accuracy than the quantization algorithm.

In view of the above advantages of the spline-based estimation algorithm, it
should prove useful in practical applications of parameter estimation as well as
of general nonlinear estimation.

REFERENCES

1. C. G. Hilborn and D. G. Lainiotis, Learning systems for minimum risk adaptive pattern
classification and optimal adaptive estimation, CSRG Technical Report No. 9, Depart-
ment of Electrical Engineering, University of Texas at Austin, November 1967.

2. D. G. Lainiotis, Adaptive mixture decomposition: A unifying approach, Proceedings of
the National Electronics Conference, 24,104-106 (1968).

3. C. G. Hilborn and D. G. Lainiotis, Optimal estimation in the presence of unknown pa-
rameters,/£'£'£' Trans. Syst. Sei. Cybernet. SSC5(1), 38-43 (January 1969).

4. D. G. Lainiotis, Optimal adaptive estimation: Structure and parameter adaptation,
Technical Report No. 74, Electronics Research Center, University of Texas, September
5, 1969.

5. F. L. Sims and D. G. Lainiotis, Recursive algorithm for the calculation of the adaptive
filter weighting coefficients, IEEE Trans. Auto. Control AC14 (2), 215-218 (April
1969).

6. D. G. Lainiotis, Optimal adaptive estimation: Structure and parameter adaptation, IEEE
Trans. Auto. Control AC16 (2), 160-170 (Aprü 1970).

7. D. G. Lainiotis, Supervised learning recursive filters for optimal structure and parameter
adaptive pattern recognition: Discrete data case, IEEE Trans. Inform. Theory IT17 (1),
(January 1971).

8. I. J. Schoenberg, On spline functions, in Inequalities, Symposium at Wright-Patterson
Air Force Base (0. Shisha, Ed.), Academic, New York, 255-291,1967.

9. M. J. Marsden, An inequality for spline functions with applications to variation dimin-
ishing spline approximation, /. Approx. Theory 3, 7-49 (1970).

10. J. P. DeFigueiredo and Y. G. Jan, Spline filters, 1971 Symposium on Nonlinear Estima-
tion and Their Applications, San Diego, 1971.

11. H. B. Curry and I. J. Schoenberg, On polya frequency functions IV: The fundamental
spline functions and their limits, J. Anal. Math. 17, 71-107 (1966).

12. M. J. Munteanu and L. L. Schumaker, Some multi-dimensional spline approximation
methods. Technical Report No. CNA-25, Center for Numerical Analysis, The University
of Texas at Austin, July 1971.

13. J. G. Deshpande and D. G. Lainiotis, Identification and control of linear stochastic
systems using spline functions. Technical Report No. 146, Electronics Research Center,
University of Texas, May 1973.

14. H. W. Sorenson, On the development of practical nonlinear filters. Inform. Sei. 7 (3/4),
253-270 (Fall 1974), this issue.

15. R. L. Sengbush and D. G. Lainiotis, Simplified parameter quantization procedure,
IEEE Trans. Auto. Control AC14 (4), 424-425 (August 1969).

PARAMETER ESTIMATION USING SPLINES                             315

16. R. S. Bucy and K. D. Senne, Digital synthesis of nonlinear filters, Automation 7 (3),
287-298 (May 1971).

17. M. Aoki, Optimization of Stochastic Systems, Academic, New York, 1967.

18. D. G. Lainiotis, S. K. Park, and R. Krishnaiah, Optimal state vector estimation for non-
Gaussian initial state-vector, IEEE Trans. Auto. Control AC16 (2), 197-198 (April
1971).

19. H. W. Sorenson and D. L. Alspach, Recursive Bayesian estimation using Gaussian sums,
Automatica 7 (4) (July 1971).

20. S. K. Park and D. G. Lainiotis, Monte-Carlo study of the optimal non-linear estimator:

Linear systems with non-Gaussian initial states. Int. J. Control 16 (4), 1029-1040
(1972).

21. J. T. Lo, Finite-dimensional sensor orbits and optimal nonlinear filtering, IEEE Trans.
Inform. Theory IT18 (5) (September 1972).

22. D. L. Alspach and H. W. Sorenson, Nonlinear Bayesian estimation using Gaussian sum
approximations, IEEE Trans. Auto. Control AC17 (4) (August 1972).

23. D. G. Lainiotis and S. K. Park, On joint detection, estimation, and system identifica-
tion: Discrete data case,/nr. J. Control 17 (3), 609-633 (1973).

24. D. L. Alspach, The use of Gaussian sum approximations on nonlinear filtering. Inform.
Sei. 7 (3/4), 271-290 (Fall 1974), this issue.

Received January, 1974</dp:raw-text><dp:aggregation-type>Journals</dp:aggregation-type><dp:version-number>S350.1</dp:version-number></dp:document-properties><cja:converted-article version="4.5.2" docsubtype="fla" xml:lang="en" xmlns:cja="http://www.elsevier.com/xml/cja/schema">
		<cja:item-info>
			<cja:jid>INS</cja:jid>
			<cja:aid>7490019X</cja:aid>
			<ce:pii xmlns:ce="http://www.elsevier.com/xml/common/schema">0020-0255(74)90019-X</ce:pii>
			<ce:doi xmlns:ce="http://www.elsevier.com/xml/common/schema">10.1016/0020-0255(74)90019-X</ce:doi>
			<ce:copyright type="unknown" year="1974" xmlns:ce="http://www.elsevier.com/xml/common/schema"/>
		</cja:item-info>
		<cja:head>
			<ce:article-footnote xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:label>☆</ce:label>
				<ce:note-para>This work was supported by the Air Force Office of Scientific Research under Grant AFOSR 69-1764A and Joint Services Electronics Programme Grant AFOSR 69-1792.</ce:note-para>
			</ce:article-footnote>
			<ce:title xmlns:ce="http://www.elsevier.com/xml/common/schema">Parameter estimation using splines</ce:title>
			<ce:author-group xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:author>
					<ce:given-name>Demetrios G.</ce:given-name>
					<ce:surname>Lainiotis</ce:surname>
				</ce:author>
				<ce:affiliation>
					<ce:textfn>Department of Electrical Engineering, State University of New York, Buffalo, New York 14214 USA</ce:textfn>
				</ce:affiliation>
			</ce:author-group>
			<ce:author-group xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:author>
					<ce:given-name>J.G.</ce:given-name>
					<ce:surname>Deshpande</ce:surname>
				</ce:author>
				<ce:affiliation>
					<ce:textfn>Department of Electrical Engineering, Indian Institute of Technology, New DelhiIndia</ce:textfn>
				</ce:affiliation>
			</ce:author-group>
			<ce:abstract class="author" xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:section-title>Abstract</ce:section-title>
				<ce:abstract-sec>
					<ce:simple-para view="all">In this paper the estimation of unknown, time-invariant parameters that, if known, completely specify a discrete, linear dynamic model with Gaussian disturbances, is considered. Following the Bayesian approach the unknown parameters are modeled as random variables with known 
						<ce:italic>a priori</ce:italic> probability density. Optimal, in the mean-square-error sense, estimates are desired. However, this requires recursive updating and storage of a non-Gaussian, and more importantly, a nonreproducing density. Therefore, exact realization of the nonlinear parameter estimators requires immense computational effort and storage capacity. To alleviate these difficulties, splines functions are used for the approximate realization of the Bayesian parameter estimation algorithm. Specifically, variation-diminishing splines are used, which are pieces of smoothly tied polynomials to approximate the 
						<ce:italic>a posteriori</ce:italic> probability density (pdf). This approximation of the pdf is specified in terms of a finite number of parameters, yielding a readily implementable approximation of the exact but unimplementable parameter estimation algorithm. A convergence theorem is obtained for the convergence of the spline algorithm to the optimal algorithm, as well as two implementable error bounds. Extensive numerical simulation indicates that the spline algorithm performs well, yielding parameter estimates very close to the true values. In summary, the salient characteristics of the proposed spline-based estimator are as follows: 
						<ce:list>
							<ce:list-item>
								<ce:label>1.</ce:label>
								<ce:para view="all">(a) it is readily implementable, its realization requiring essentially a bank of Kalman filters and integration of piece-wise polynomial functions;</ce:para>
							</ce:list-item>
							<ce:list-item>
								<ce:label>2.</ce:label>
								<ce:para view="all">(b) the spline algorithm is based essentially on an imbedded quantization algorithm, namely, the one corresponding to the bank of Karman filters used;</ce:para>
							</ce:list-item>
							<ce:list-item>
								<ce:label>3.</ce:label>
								<ce:para view="all">(c) as such, the spline estimator has greater computational requirements than the associated quantization algorithm; but</ce:para>
							</ce:list-item>
							<ce:list-item>
								<ce:label>4.</ce:label>
								<ce:para view="all">(d) the spline algorithm is more accurate than the quantization algorithm. In view of the above, the spline-based estimator should prove useful in practical applications of parameter estimation as well as of general nonlinear estimation.</ce:para>
							</ce:list-item>
						</ce:list>
					</ce:simple-para>
				</ce:abstract-sec>
			</ce:abstract>
		</cja:head>
		<cja:tail>
			<ce:bibliography view="all" xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:section-title>References</ce:section-title>
				<ce:bibliography-sec>
					<ce:bib-reference id="BIB1">
						<ce:label>1.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>C.G.</ce:given-name>
										<ce:surname>Hilborn</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Learning systems for minimum risk adaptive pattern classification and optimal adaptive estimation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>CSRG Technical Report No. 9</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>November 1967</sb:date>
									<sb:publisher>
										<sb:name>Department of Electrical Engineering, University of Texas</sb:name>
										<sb:location>Austin</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB2">
						<ce:label>2.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Adaptive mixture decomposition: A unifying approach</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Proceedings of the National Electronics Conference</sb:maintitle>
											</sb:title>
											<sb:volume-nr>24</sb:volume-nr>
										</sb:series>
									</sb:book-series>
									<sb:date>1968</sb:date>
								</sb:edited-book>
								<sb:pages>
									<sb:first-page>104</sb:first-page>
									<sb:last-page>106</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB3">
						<ce:label>3.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>C.G.</ce:given-name>
										<ce:surname>Hilborn</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal estimation in the presence of unknown parameters</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Syst. Sci. Cybernet.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>SSC5</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>1</sb:issue-nr>
									<sb:date>January 1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>38</sb:first-page>
									<sb:last-page>43</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB4">
						<ce:label>4.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal adaptive estimation: Structure and parameter adaptation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Technical Report No. 74</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>September 5, 1969</sb:date>
									<sb:publisher>
										<sb:name>Electronics Research Center, University of Texas</sb:name>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB5">
						<ce:label>5.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>F.L.</ce:given-name>
										<ce:surname>Sims</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Recursive algorithm for the calculation of the adaptive filter weighting coefficients</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Auto. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC14</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>2</sb:issue-nr>
									<sb:date>April 1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>215</sb:first-page>
									<sb:last-page>218</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB6">
						<ce:label>6.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal adaptive estimation: Structure and parameter adaptation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Auto. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC16</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>2</sb:issue-nr>
									<sb:date>April 1970</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>160</sb:first-page>
									<sb:last-page>170</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB7">
						<ce:label>7.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Supervised learning recursive filters for optimal structure and parameter adaptive pattern recognition: Discrete data case</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Inform. Theory</sb:maintitle>
										</sb:title>
										<sb:volume-nr>IT17</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>1</sb:issue-nr>
									<sb:date>January 1971</sb:date>
								</sb:issue>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB8">
						<ce:label>8.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>I.J.</ce:given-name>
										<ce:surname>Schoenberg</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On spline functions</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:editors>
										<sb:editor>
											<ce:given-name>O.</ce:given-name>
											<ce:surname>Shisha</ce:surname>
										</sb:editor>
									</sb:editors>
									<sb:title>
										<sb:maintitle>
											<ce:italic>Inequalities</ce:italic>, Symposium at Wright-Patterson Air Force Base
										</sb:maintitle>
									</sb:title>
									<sb:date>1967</sb:date>
									<sb:publisher>
										<sb:name>Academic</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:edited-book>
								<sb:pages>
									<sb:first-page>255</sb:first-page>
									<sb:last-page>291</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB9">
						<ce:label>9.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>M.J.</ce:given-name>
										<ce:surname>Marsden</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>An inequality for spline functions with applications to variation diminishing spline approximation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>J. Approx. Theory</sb:maintitle>
										</sb:title>
										<sb:volume-nr>3</sb:volume-nr>
									</sb:series>
									<sb:date>1970</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>7</sb:first-page>
									<sb:last-page>49</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB10">
						<ce:label>10.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.P.</ce:given-name>
										<ce:surname>DeFigueiredo</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>Y.G.</ce:given-name>
										<ce:surname>Jan</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Spline filters</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:title>
										<sb:maintitle>1971 Symposium on Nonlinear Estimation and Their Applications</sb:maintitle>
									</sb:title>
									<sb:conference>San Diego</sb:conference>
									<sb:date>1971</sb:date>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB11">
						<ce:label>11.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>H.B.</ce:given-name>
										<ce:surname>Curry</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>I.J.</ce:given-name>
										<ce:surname>Schoenberg</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On polya frequency functions IV: The fundamental spline functions and their limits</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>J. Anal. Math.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>17</sb:volume-nr>
									</sb:series>
									<sb:date>1966</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>71</sb:first-page>
									<sb:last-page>107</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB12">
						<ce:label>12.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>M.J.</ce:given-name>
										<ce:surname>Munteanu</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>L.L.</ce:given-name>
										<ce:surname>Schumaker</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Some multi-dimensional spline approximation methods</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Technical Report No. CNA-25</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>July 1971</sb:date>
									<sb:publisher>
										<sb:name>Center for Numerical Analysis, The University of Texas</sb:name>
										<sb:location>Austin</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB13">
						<ce:label>13.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.G.</ce:given-name>
										<ce:surname>Deshpande</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Identification and control of linear stochastic systems using spline functions</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Technical Report No. 146</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>May 1973</sb:date>
									<sb:publisher>
										<sb:name>Electronics Research Center, University of Texas</sb:name>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB14">
						<ce:label>14.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>H.W.</ce:given-name>
										<ce:surname>Sorenson</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On the development of practical nonlinear filters</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Inform. Sci.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>7</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>3/4</sb:issue-nr>
									<sb:date>1974</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>253</sb:first-page>
									<sb:last-page>270</sb:last-page>
								</sb:pages>
							</sb:host>
							<sb:comment>Fall</sb:comment>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB15">
						<ce:label>15.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.L.</ce:given-name>
										<ce:surname>Sengbush</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Simplified parameter quantization procedure</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Auto. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC14</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>4</sb:issue-nr>
									<sb:date>August 1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>424</sb:first-page>
									<sb:last-page>425</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB16">
						<ce:label>16.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.S.</ce:given-name>
										<ce:surname>Bucy</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>K.D.</ce:given-name>
										<ce:surname>Senne</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Digital synthesis of nonlinear filters</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Automatica</sb:maintitle>
										</sb:title>
										<sb:volume-nr>7</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>3</sb:issue-nr>
									<sb:date>May 1971</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>287</sb:first-page>
									<sb:last-page>298</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB17">
						<ce:label>17.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>M.</ce:given-name>
										<ce:surname>Aoki</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimization of Stochastic Systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:book>
									<sb:date>1967</sb:date>
									<sb:publisher>
										<sb:name>Academic</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB18">
						<ce:label>18.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>S.K.</ce:given-name>
										<ce:surname>Park</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>R.</ce:given-name>
										<ce:surname>Krishnaiah</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal state vector estimation for non-Gaussian initial state-vector</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Auto. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC16</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>2</sb:issue-nr>
									<sb:date>April 1971</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>197</sb:first-page>
									<sb:last-page>198</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB19">
						<ce:label>19.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>H.W.</ce:given-name>
										<ce:surname>Sorenson</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>D.L.</ce:given-name>
										<ce:surname>Alspach</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Recursive Bayesian estimation using Gaussian sums</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Automatica</sb:maintitle>
										</sb:title>
										<sb:volume-nr>7</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>4</sb:issue-nr>
									<sb:date>July 1971</sb:date>
								</sb:issue>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB20">
						<ce:label>20.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>S.K.</ce:given-name>
										<ce:surname>Park</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Monte-Carlo study of the optimal non-linear estimator: Linear systems with non-Gaussian initial states</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Int. J. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>16</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>4</sb:issue-nr>
									<sb:date>1972</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>1029</sb:first-page>
									<sb:last-page>1040</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB21">
						<ce:label>21.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.T.</ce:given-name>
										<ce:surname>Lo</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Finite-dimensional sensor orbits and optimal nonlinear filtering</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Inform. Theory</sb:maintitle>
										</sb:title>
										<sb:volume-nr>IT18</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>5</sb:issue-nr>
									<sb:date>September 1972</sb:date>
								</sb:issue>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB22">
						<ce:label>22.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.L.</ce:given-name>
										<ce:surname>Alspach</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>H.W.</ce:given-name>
										<ce:surname>Sorenson</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Nonlinear Bayesian estimation using Gaussian sum approximations</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Auto. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC17</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>4</sb:issue-nr>
									<sb:date>August 1972</sb:date>
								</sb:issue>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB23">
						<ce:label>23.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>S.K.</ce:given-name>
										<ce:surname>Park</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On joint detection, estimation, and system identification: Discrete data case</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Int. J. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>17</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>3</sb:issue-nr>
									<sb:date>1973</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>609</sb:first-page>
									<sb:last-page>633</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB24">
						<ce:label>24.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.L.</ce:given-name>
										<ce:surname>Alspach</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>The use of Gaussian sum approximations on nonlinear filtering</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Inform. Sci.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>7</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>3/4</sb:issue-nr>
									<sb:date>1974</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>271</sb:first-page>
									<sb:last-page>290</sb:last-page>
								</sb:pages>
							</sb:host>
							<sb:comment>this issue.</sb:comment>
						</sb:reference>
					</ce:bib-reference>
				</ce:bibliography-sec>
			</ce:bibliography>
		</cja:tail>
	</cja:converted-article></doc:document>
