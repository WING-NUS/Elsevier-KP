<doc:document xmlns:doc="http://www.elsevier.com/xml/document/schema"><rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"><rdf:Description rdf:about="http://dx.doi.org/10.1016/0020-0255(74)90020-6"><dc:format xmlns:dc="http://purl.org/dc/elements/1.1/">application/xml</dc:format><dc:title xmlns:dc="http://purl.org/dc/elements/1.1/">Partitioned estimation algorithms, II: Linear estimation</dc:title><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/"><rdf:Seq><rdf:li>Demetrios G. Lainiotis</rdf:li></rdf:Seq></dc:creator><dc:description xmlns:dc="http://purl.org/dc/elements/1.1/">Information Sciences 7 (1974) 317-340. doi:10.1016/0020-0255(74)90020-6</dc:description><prism:aggregationType xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">journal</prism:aggregationType><prism:publicationName xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">Information Sciences</prism:publicationName><prism:copyright xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">Copyright © unknown. Published by Elsevier Inc.</prism:copyright><dc:publisher xmlns:dc="http://purl.org/dc/elements/1.1/">Elsevier Inc.</dc:publisher><prism:issn xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">0020-0255</prism:issn><prism:volume xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">7</prism:volume><prism:coverDisplayDate xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">1974</prism:coverDisplayDate><prism:coverDate xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">1974</prism:coverDate><prism:pageRange xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">317-340</prism:pageRange><prism:startingPage xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">317</prism:startingPage><prism:endingPage xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">340</prism:endingPage><prism:doi xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">10.1016/0020-0255(74)90020-6</prism:doi><prism:url xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">http://dx.doi.org/10.1016/0020-0255(74)90020-6</prism:url><dc:identifier xmlns:dc="http://purl.org/dc/elements/1.1/">doi:10.1016/0020-0255(74)90020-6</dc:identifier></rdf:Description></rdf:RDF><dp:document-properties xmlns:dp="http://www.elsevier.com/xml/common/doc-properties/schema"><dp:raw-text>INFORMATION SCIENCES 7, 317-340 (1974) 317

Partitioned Estimation Algorithms, II: Linear Estimation

DEMETRIOS G. LAINIOTIS

Department of Electrical Engineering, State University of New York at Buffalo,
Buffalo, New York 14214

ABSTRACT

In a radically new approach to linear estimation, Lainiotis [33, 36-37, 52-53], using the
"partition theorem"-an explicit Bayes theorem-obtained fundamentally new linear
filtering and smoothing algorithms both for continuous as well as discrete data. The new
algorithms are given in explicit, integral expressions of a "partitioned" form, and in terms
of decoupled forward filters. The "partitioned" algorithms were shown to be especially
advantageous from a computational as well as from an analysis standpoint. They are
essentially based on the decomposition of the innovations into partial or conditional
innovations and residuals.

In this paper, the "partitioned" algorithms are shown to be the natural framework in
which to study such important concepts as observability, controllability, unbiasedness,
and the solution of Riccati equations. Specifically, in this paper, the "partitioned" algo-
rithms are re-examined yielding further insight as well as several significant new results on:

(a) unbiased estimation and filter initialization procedures; (b) stochastic observability and
stochastic controllability; (c) the interconnection between stochastic observability. Fisher
information matrix, and the Cramer-Rao bound; (d) estimation error-bounds; and most
importantly, (e) computationally effective "partitioned" solutions of time-varying matrix
Riccati equations. In fact, all of the above results have been obtained for general, time-
varying, lumped, linear systems.

In addition, it is shown that previously established smoothing algorithms, such as the
Meditch differential algorithm and the Kailath-Frost total innovation algorithm, are readily
obtained from the "partitioned" algorithms. The properties of the "partitioned" algo-
rithms are obtained, thoroughly examined, and compared to those of other algorithms.

1. INTRODUCTION

The linear estimation problem considered is briefly described by the following
model equations and statement of objective: Specifically, the model is given by
the equations:

dxjû=F(t)x(.t)+u(t),                       (1)

z(t)=H(t)x(t)+u(t),                       (2)
© American Eisevier Publishing Company, Inc., 1974

318                                          DEMETRIOS G. LAINIOTIS

where x (0 and z (t) are the n and w-dimensional state-vector and observation
processes, respectively; {"(0} and {ü(/)} are the independent plant and observa-
tion, zero-mean, white-gaussian noise random processes, with covariances Q(t),
and R (t), respectively. The initial state vector, x (to ) = Xo, is gaussian with
actual meanJCo and covariance matrix F(0/0). Moreover,^ is independent of
{u(t)},wd{V(t)],foit&gt;to.

Given the measurement record, \i = {x(r); r £ (to, t)}, the optimal, in the
mean-square-error-sense, filtered estimate x(t/t) ofx(r) and smoothed estimate
x (rit) ofx (r), for to &lt; r &lt; t, are desired.

The above lumped, linear estimation problem has attracted considerable
attention in the past decade [1-33]. Specifically, the filtering and prediction
cases of the problem were first solved by Kalman [ 1 ], and Kalman and Bucy
[2] in their classic papers [1, 2,3]. Kalman and Bucy's results are most note-
worthy not only for their novelty but also because of Kalman and Bucy's
thorough and complete solution of filtering and prediction and related aspects
of the problem such as the stability of the Kalman-Bucy filter. Subsequently,
others [4-11 ], Ho [4, 5], Ho and Lee [6], Rauch et al. [7], Athans and Tse
[8], Stear [9], Stubberud [9], Kailath [10], Meditch [11] obtained essentially
the same algorithms, each using a different approach or considering different
aspects of the problem. Most recently, Kailath [12] obtained a new and compu-
tationally efficient algorithm for obtaining the Kalman filter gains for constant
linear systems using essentially the Abarzumian-Chandrasekharan solution of
the Riccati equation. This algorithm is related to the Levinson [12a] algorithm
for discrete systems.

Similarly, the linear smoothing problem has attracted an even greater
attention [13-32]. Specifically, it has been solved by Bryson and Frazier [13],
Rauch [14], Lee [15], Cox [16], Mayne [17], Fraser [18], Meditch [19-21],
Bryson and Henrikson [22], Mehra [23], Baggeroer [24], Kailath and Frost
[25], Kwakernaak [26], Zachrisson [27], Nishimura [28], Wilman [29],
Anderson et al. [30, 31], Biswas and Mahalanobis [32].

In a radically different approach to the linear estimation problem, Lainiotis
[36-37], using the "partition theorem"an explicit Bayes theorem applicable
in general to nonlinear as well as linear estimationobtained fundamentally new
optimal linear filtering and smoothing algorithms in terms of explicit, integral
expressions in a "partitioned" form. The "partitioned" estimation algorithms
were shown [36-37] to have several important properties.

In this paper, the "partitioned" estimation algorithms are re-examined
yielding several significant new results, namely: (a) the conditions for unbiased
estimation and related results on filter initialization procedures; (b) the condi-
tions for stochastic observability and controllability for time-varying systems;

(c) the fundamental interconnection between stochastic observability, Fisher
information matrix, and the Cramer-Rao bound; (d) estimation upper and lower
bounds; and (e) "partitioned" and computationally efficient solutions of

PARTITIONED ESTIMATION ALGORITHMS II                            319

Riccati equations. It must be noted that the above results were obtained for the
general, time-varying linear system given by Eqs. (1-2).

In addition, it is shown that previously established linear smoothing algo-
rithms, such as the Meditch differential algorithm [21], the Mayne-Fraser-Mehra
two-filter differential algorithm [17, 18, 23] as well as the Kailath-Frost [25]
"innovation" algorithm are readily obtained from the "partitioned" algorithms.
Further, the properties of the "partitioned" algorithms are obtained, thoroughly
examined, and compared to those of previous algorithms.

2. "PARTITIONED" ESTIMATION ALGORITHMS

The continuous-data, "partitioned" smoothing and filtering algorithms are
given, Lainiotis [36-37], by the following theorems

THEOREM 1 : "PARTITIONED" SMOOTHING ALGORITHM The
smoothed estimate and the corresponding error-covariance matrix are given by

S (Tit) = P(rlt) [Mo (t, r) + P-1 (T/T);? (T/T)] ,              (3)
and

W) = [ ^O1 {t. T) + P-1 (T/T)] -1 ,                    (4)

where x (r/r) and P(Tlr) are the filtering estimate and filtering error-covariance
matrix, respectively; Mo(t, r) and Vo1 (t, r) are defined as

Mo(t, r) =  T ^l(a, r^W1 (a)zo(a)da,               (5)

Fo1 (t. T) =t $?(o, T^W1 (a)H(o)&lt;S&gt;o(a, T)da,         (6)

where ?o(t) = z(t) - H(t)xy(tlt), and Xo(t/t), t &gt; T, is given by the Kalman
filter equations

dxom=F(t)xc,(t/t)+K^t)z^t),                 (7)
with initial condition x (r/r) = 0; where

^(O^oO/^ró/r'M,              (8)

and
dpoj^t}- = F(t)P^(t/t) + P^t/DF^t) + Q(t)

- P^t/tW^R-1 (t)ff(t)Po(t/t), (9)
with initial condition P 0(7 IT) = 0.

320                                          DEMETRIOS G. LAINIOTIS

The transition matrix &lt;Ï&gt;o(A T) of the above Kalman filter corresponds to the
dynamic matrix F(t) - PoCt/t)!!'1'^'1 (t)H(f).

Proof. The explicit proof of this theorem was given by Lainiotis [36] as a
special case of the "partition" theorem, Lainiotis [34-39]. Hence the proof is
omitted.

The "partitioned" filtering algorithm is given in the following theorem.

THEOREM 2: "PARTITIONED" FILTERING ALGORITHM. The filtering
estimate x (t/t) and the filtering error-covariance matrix P(t/t) are given by

x(t/t)=Xo(t/t)-^S&gt;o(t,T)x(T/t)                      (10)

and

P(t/t) = Po(t/t) + $o(r, T)P(r/t)^(t, r),               (11)
where t &gt; r, and all quantities are as defined in Theorem 1.

Proof. The proof was given in Lainiotis [36], hence it is omitted.

Remarks.

(i) We note that the "partitioned" smoothing Eqs. (3-4) are explicit
expressions of the smoothed estimate and covariance matrix, respectively, in
terms of the initial conditions, {x (r/r), PCrl?)} at time T, and the quantities
[Mo(t, r), Vo1 (t, r)}. The initial conditions at r{x(T/T),/'(r/T)} are obtained
from a forward Kalman filter operating on the data {z(a); a £ (to, r)} and
with knowledge of the initial conditions {x(tolto), P(tolto)] at to. The
quantities [Mo(t, r), Fo1 (.t, r)} are given explicitly in integral form, in terms
of the forward filtered estimate Xo(ala), a &gt; T, and the corresponding Kalman
filter transition matrix &lt;Ï&gt;o(a, r), both of which are independent of the initial
conditions {J?(r/T),P(T/r)} at T. Namely, they depend only on the data
{z(o); o £ (r, t)}. Thus, we see that there is an explicit "partitioning" of
x(r/t) and P(r/t) into two parts, one reflecting the information in the data from ty
to r(as well as the initial conditions at ty) as summarized in the initial conditions
at T, and the other reflecting the information in the data from T to t, indepen-
dently of the past information.

In fact, it can be readily shown (see also the pertinent remarks m Section III)
that x (r/0, as given in Eq. (3), is the optimal linear combination of the two
filtered estimates based on {z(o); oG (to, T);x(tolto),P(tolto)} and {2(0);

o(T, /)}, respectively.

(ii) Moreover, we note that the "partitioned" filtering algorithms, Eqs.
(10-11), are also decomposed (or "partitioned") into two parts, namely Xo(t/t),
and &lt;t&gt;o (t, r) x (T/t), where t &gt; r. The first part, Xy(t/t), pertains to estimation
with assumed zero initial conditions at T, namely with the state x(r) "anchored"
at zero. The second part, $o (t, r), pertains to smoothing of the initial state
x(r), where x(r) = Xy may be viewed as a parameter. Thus, minimum variance

PARTITIONED ESTIMATION ALGORITHMS II                             321

filtering has been partitioned into a filtering problem with known initial condi-
tions, namely zero, and a parameter estimation pertaining to x,-.1

Further insight into the "partitioned" nature of the algorithms can be
obtained by utilizing the "adaptive" framework in formulating the problem. In
fact, the "partitioned" algorithms of linear as well as nonlinear estimation
[34-39] have been readily obtained via the "partition" theorem [34-39] first
obtained by the author in connection with adaptive estimation [34-35].

Specifically, the "adaptive" formulation simply consists in viewing the initial
state-vector as an unknown and random parameter of the model [35-36]. In
this framework, the "partitioned" filtering can be seen to be decomposed into
the "non-adaptive" partJ?o(?/?) (no adaption is required since the "parameter"
XT is assumed known) and into the "adaptive" part $o (t, r)x(r/t), which
incorporates the adaptation (or updating) with respect to the unknown param-
eter, namely the initial state XT.

(iii) Further insight into the basic nature of "partitioned" filtering can be
obtained by comparing the "partitioned" filtering equations (10-11) with the
corresponding equations for x(t) mdP(t)=E {^(Ox7^?)}, as obtained from the
model equations (1-2). The latter are given by

x(t)=Xo(t)+&lt;î&gt;(t,r)x(T),                   (12)

and

where

P(0 = Po(0 + $(?, T)P(rlT) &lt;ï&gt;(t, r),

(13)

xo(t)=ï &lt;î&gt;(t,a)u(a)da, (14)

and

Po(0=   &lt;S&gt;(t,a)Q(a)^(t,o)da.               (15)

T

Comparing Eqs. (12-13) to Eqs. (10-11) of "partitioned" filtering, we note
a striking similarity between them. This similarity becomes even more apparent
by noting that Xo(t/t) and Po(t/t) may be given (for details see section V) as
follows

^oWO= &lt;î&gt;o(t,a)z(a)do, (16)

Zo (t) constitutes a partial or conditional innovation. Thus, the "partitioned" algo-
rithms are based on a "partitioning" or decomposition into conditional innovations and
residuals.

322                                          DEMETRIOS G. LAINIOTIS

P^(t/t) = f $o (t, a)A (t, o)&lt;S&gt;ï(t, a)do,             (17)

-T

where z(a)=Ko(,a)z(a), and .4 (r, o) = ß(o) + ^0(0)^(0)^0 (o).

We note that both sets of equations, Eqs. (12-13) and Eqs. (10-11) are
"partitioned" into a part due to excitations and into a part due to initial
conditions. In fact, the later part constitutes a "predicted" estimate ofx(f)
based on the best knowledge of the initial state. Specifically, the optimum
predicted estimate of x(t) given x(r) is $(/, T)x(r), while the optimum predicted
estimate given \- is $(f, r)x (T/r). Similarly, *ï&gt;o(^ 'r)x (r/t) constitutes the
updated "predicted" estimate ofx(t) based on the smoothed estimate of the
initial state x (r), the prediction dynamics being $o(^&gt; r) instead of the usual
4&gt;(ï, r). Namely, instead of the a priori predicted estimate &lt;!&gt;(?, r)x (r/r), we
now have the smoothed (or updated by the use of ( {z(o); o £ (r, t)]) predicted
estimate ^o(t, r)x (rif). The same remarks also apply to the covariance matrices
P(t) and P(t/t). Both of the above results are a direct consequence of the
superposition property of linear systems.2

(iv) It will be seen in later sections that the above interpretations of the
nature of the "partitioned" algorithms constitute the intrinsic and basic expla-
nation for the ease with which the important observability, controllability and
unbiasedness results are obtained through the use of the "partitioned" algo-
rithms. Namely, the "partitioned" algorithms are the natural framework in
which to study the above concepts.

(v) The "partitioned" filtering algorithm is essentially the first new basic
filtering formula after the Kalman filter. However, note that "partitioned"
filtering requires for its realization a Kalman filter as its basic block. This
indicates the fundamental and basic nature of the Kalman filter structure. In
fact, it has been shown, Ho [5], that the Kalman filter belongs to the general
class of stochastic approximation algorithms.

(vi) On the matter of the realizability of the "partitioned" filtering and
smoothing algorithms, we note that they are both given in terms ofxo (t/t\ or
equivalently in terms of the partial innovations Zo(r), hence both the filter and
the smoother may be realized m parallel, and in real-time, in terms of the for-
ward filtered estimate Xo (t/t).

To complete the presentation of the "partitioned" algorithms, and for later
use, the differential form of the "partitioned" smoothing algorithm is obtained
by differentiation of Eqs. (3-4). It is given in the following corrollary

COROLLARY 1.

dx^/t) = P(T/t)&lt;S&gt;ï(t, T)/T(^-1 (t) [Zo W - H(t)&lt;S&gt;o(t, T)X (T/t)],   (l 8)

dt

Thus, the above partitioning may be viewed as "predictive decomposition" for time-
varying systems!

PARTITIONED ESTIMATION ALGORITHMS II                            323

and

dpm = -P(T/0&lt;(r, rWW1 {t)H{t)^(t, r)P(T/t).      (19)

3. PREVIOUS RESULTS: COMPARISONS AND INTERCONNECTIONS

It is of interest to compare and contrast the properties of the "partitioned"
linear estimation algorithms to previously derived filtering and smoothing algo-
rithms, such as the Kalman-Bucy filter algorithm [2], the Meditch smoothing
algorithm [21], the Kailath-Frost smoothing formula [25], and the Mayne-
Fraser-Mehra smoothing formula [17,18,23]. Several additional interesting
results will result from this comparison, as well as it will be demonstrated how
readily the above previous results, as well as related ones, can be obtained by
simple manipulations of the "partitioned" formulas.

The desired derivations, comparisons and interconnections are given in the
following.

KALMAN-BUCY FIL TER

Given the "partitioned" filtering Eqs. (10-11) we readily obtain the corre-
sponding Kalman-Bucy filter equations by straight forward differentiation of
Eqs. (10-11) with respect to t. This indicates the strong interconnection
between the Kalman-Bucy and the "partitioned" formulas.

As indicated previously, the "partitioned" filtering algorithm is essentially
the first new basic filtering formula after the Kalman filter. However, it has as
its basic building block a Kalman filter. The latter is indicative of the funda-
mental nature of the Kalman filter. However, the partitioned filter is far supe-
rior, numerically, to the Kalman filter. Moreover, the "partitioned" formula is
the natural framework for analysis, especially as it pertains to sensitivity
analysis with respect to the choice of initial conditions, and to the study of such
important aspects as observability, controllability, unbiasedness, and estimation
error-bounds.

A comparison of the "partitioned" filtering Eqs. (10-11) to the correspond-
ing Kalman-Bucy expressions [2], yields the following interesting result.

COROLLARY 2.

^(f,T)P(T/T)=$o(/,T)^(T/r) (20)

or

W, r) = $o0, r) - $o(r, WO, r)W), (21)

where ^^ (t, r) is the transition matrix of the Kalman-Bucy filter dynamic
matrix FK (t) = F(t) - .P(^/Or(O.R'"l (t)H(t), and ^o(t, r) is the transition

324                                            DEMETRIOS G. LAINIOTIS

matrix of the "partitioned" filter dynamic matrix Fo(t) = F(t) - P^t/t)^^) 
R-^tWt).

Proof. The proof is straightforward, hence it is omitted.

Remarks.

(i) The property indicated by Eq. (20) is an interesting one and of impor-
tance in understanding the nature of "partitioned" estimation. To the author's
knowledge this property has not been boserved before. Eq. (20) essentially
states that ifP(T/r) is used as a priori knowledge then the prediction dynamics
to use is *i&gt;Jc(' , '), while if the smoothed covariance is used the prediction
dynamics to use is 4&gt;o(- , ).

(ii) Further insight into the property is obtained by studying its equivalent
version in Eq. (21). Namely, we note that $^(-, ) which corresponds to the
dynamic matrix F^t), where F^ (t) = Fo(t) + AF(/) is the sum of the matrices
FO (0 and AF(t) = -4&gt;o (t, r)  P(T/O$;(/, r^^R'1 (t)H(t), is given as the
difference of ^&gt;o(t, r) and an explicit integral expression of ^o(t, r). To the
author's knowledge this is the first time that this explicit result was obtained.
It should be contrasted to a related result ofKinariwala [40] given in a Neumann
series and not m closed form as in Eq. (21).3

MEDITCHSMOOTHING FORMULA

Using Eq. (20) in the differential form of the "partitioned" algorithm Eqs.
(18-19) (that is substituting PÇT/T^K (f, r} îoiP(T/t)^o(t, r)) and noting that
Ïo(t) - H(t)&lt;S&gt;o(t, r)Jc(T/0 = z(0 - H(t)x(tlt) = z(t), we readily obtain the
Meditch [21] differential smoothing formula as given in the following corollary.

COROLLARY 3.

dx(T/t)
at

= P(r/t)^ (t/^H^R-1 (t)Z{t) (22)

dpMl= -P(T/T)^ (t, r)/^(0/?-1 (t)H(t)&lt;S&gt;K(t, T)P(T/T),     (23)
0?-1 &gt; T and with initial conditions x (j Ir), and P(r/T), respectively.

KAILA TH-FROST "INNO VA TION" SMOOTHING FORMULA

Integrating Meditch's smoothing formulas, Eqs. (22-23), we readily obtain
the Kailath-Frost "innovation" smoothing formulas [25], as given in the
following corollary.4

3This interpretation of Eq. (21) is useful in obtaining solutions of Fredholm equations

[50].

It may be noted that this version of the Kailath-Frost formula in which the matrices
M{') and V(-) appear explicitly, was first given and interpreted by Lainiotis [37].

PARTITIONED ESTIMATION ALGORITHMS II                             325
COROLLARY 4.

x (r/t) = x (r/r) + P(T/r)M(t, r)                  (24)
and

P(r/t) = HT/T) - P(T/T) V-1 (t, T)P(T/T),              (25)
where

rt

M(t, r) =   $1 (o, T^o)^-1 (o)Z(o)do,            (26)

'T

F-1 (f, r) = f «ti (o, ^H^R-1 ((7)1^(0, r)u?o,         (27)

'T

and Z (t) = Z (t) - H(t)x (t/t), and x (t/t) is the filtered estimate given by a
Kalman filter.

Remarks'

(i) From Eqs. (24-27), we note that, as is the case with the "partitioned"
smoothing Eqs. (3-6), the Kailath-Frost smoothing expressions are also given in
terms of the forward filtered estimates, x (r/r) and x (0/0), and their error-
covariance matrices, P(T/T) and P(a/a), a &gt; T. However, we also note an
important difference between the two; namely in the Kailath-Frost expressions,
M(t, r) and V~1 (t, r) are not independent of the initial conditions, x (r/r) and
P(T/r) as are the corresponding expressions My(t, r) and Fo1 (t, r) in the
"partitioned" smoothing equations. It was pointed out earlier that this "parti-
tioning of the information and the independence ofMo(t, T) and Vo1 (t, r) from
initial conditions is the essential factor in the study ofunbiasedness, observa-
bility, controllability, initialization of the Kalman filter, and other related
matters.

(ii) We note, however, a seeming disadvantage of the "partitioned" algo-
rithm vis-a-vis the Kailath-Frost formula. Namely, the "partitioned" algorithm
requires two decoupled forward Kalman filters for its realization (one initialized
at to with initial conditions [x (to/to); P(to/to)} and operating on dataz(a),
o S (to, r), and the other is initialized at T with a zero initial condition and
operating on data z (o), o £ (r, t)), while the Kailath-Frost smoother requires
only one Kalman filter initialized at to with initial conditions {x (to I to);

P(to/to)}, and operating on the data z(o), a £ (ty, t), i.e., data from the whole
interval. Essentially, then the Kailath-Frost algorithm requires the solution of
one Riccati equation for the whole interval {to, t} with initial condition
P(to/to), while the "partitioned" algorithm decomposes the above Riccati
equation solution into two decoupled Riccati equation solutions, that is, one
for {ty, r} with initial condition P(to/to) and the other for the interval {r, t}
with zero initial conditions. This not only does it not mean greater computa-

326                                          DEMETRIOS G. LAINIOTIS

tional requirements for the "partitioned" algorithm, but it also proves advanta-
geous from a numerical standpoint. Specifically, in the numerical solution of
the Riccati equation, the "partitioned" algorithm avoids numerical integration
of the Ricati equation over the longer time interval {to,t} by partitioning it
into two decoupled subintervals, resulting in smaller errors in the second sub-
interval than the Kailath-Frost algorithm which performs the numerical integra-
tion continuously over the whole interval, thus propagating m the second sub-
interval the numerical errors incurred m the first subinterval. The above
statements are made more precise and proven conclusively in reference [50].
In fact, it is shown in [50] that partitioning the total data interval into several
subintervals and using the "partitioned" algorithms in each subinterval results
in smaller errors and alleviates the difficulties associated with numerically inte-
grating the general ill-conditioned, time-varying, continuous Riccati equation.

MA YNE-FRASER-MEHRA SMOOTHING FORMULA

Of the previous results on linear smoothing, the two-filter formula of Mayne
[17], Fraser [18], and Mehra [23] is the closest to the "partitioned" algorithm.
In fact, the two formulas share the basic "partitioned" nature with, however, a
fundamental difference. To facilitate a comparison between the two, the
Mayne-Fraser-Mehra formula is given in the following theorem [23].

THEOREM 3.

x (T/O = P(r/t) [PÎ1 (T)JCb(r) + P-1 (T/T)X (T/T)] ,            (28)
and

P(T/t)=[P-t,l(r)+P-l(T/T)]-l,                 (29)

where x (T/T), and P(r/T) are the forward filtered estimate and covariance matrix,
respectively, as defined previously. Xi,(t) and Pi,(t) are the backward filtered
estimate and covariance, respectively.

The quantities y (r) = P],1 (r)xi,(T), and Pï,1 (r) are given, Mehra [23], by the
backward Kalman filter described by the differential equations

dy^}- = - [Hr) - QW (T)] ^ (r) - H^R-1 (T)Z(T)      (30)

d-r

with final condition y(t)=0; and
dpb{r}- = -P^ (T)F(T) - FT(T)Pï\ (T) + PÎ1 (r)Q(T)P,1 (T) - H^R-1 (r)H(r\

"T

(31)
with final condition P^1 (/) = 0.

Remarks.

(0 We note again that both smoothing formulas of Eqs. (3-4) and Eqs.
(28-29) are "partitioned" in the same manner; hence they have essentially the

PARTITIONED ESTIMATION ALGORITHMS II                             327

same properties with one fundamental difference. Specifically, Eqs. (28-29)
are given in terms of the forward filtered estimate and covariance matrix, x(T/r)
and P(r/T), respectively, as defined previously, and the backward filtered esti-
mate and covariance matrix Xi,(r) and.P(,(7")) respectively. Thus, the important
difference between Eqs. (3-4) and Eqs. (28-29), is that the former are given in
terms of two forward Kalman filter estimates, while the latter are given in terms
of one forward and one backward Kalman filter.

Thus, while both formulas permit partitioning of the data into two parts and
parallel processing of the two parts, only the "partitioned" smoothing formula
may be utilized also in real-time processing.

(ii) Further comparison of the two formulas yields an additional result.
Specifically, we note that in view of the equivalence of the two smoothing
formulas, Vo1 (t, r) = P^ (r). Namely, we observe that P],1 (r) is given in terms
of the backward Riccati Eq. (31 ) with final condition Pj,1 (?) = 0, or it is given,
in its Fo1 (/, T") reincarnation, by Eq. (6), which requires for its evaluation the
solution of the forward Riccati Eq. (9) with initial condition Py (r/r) = 0. Thus,
the problem of solving the backward Riccati Eq. (31) was transformed to that
of solving the forward Riccati Eq. (9) and the linear matrix equation for the
transition matrix ^o(t, f). This was first given by Lainiotis [36, 37].

The above result is of interest both in estimation as well as in deterministic
and stochastic control. Its particular significance is elaborated upon further in a
following section.

(iii) There is an additional fundamental difference between the Mayne-
Fraser and the "partitioned" formulas. Namely, while the Mayne-Fraser formula
is not valid when the signal process y{t) = H{t)x(t) depends on past { V(r)}, the
"partitioned" formula remains valid. This is an important consideration es-
pecially in cases where feedback is used, and also in the study of colored noise
problems.

4. UNBIASED ESTIMATION

Using the "partitioned" algorithms we readily obtain the bias and the actual
error-covariance matrix of the smoothed estimate, resulting from initializing the
smoothing algorithms with the nominal initial state-vector mean x (r/r) and
variance P(r/r) instead of the actual corresponding quantities x(r/T), and VCr/r),
respectively.

The bias and actual variance are given in the following corollary of Theorem
1.

COROLLARY 5. The bias, defined as e(r/a) = E[x(r) - x (T/o)/o], is given by
e(r//) = P(r/t)P-1 (r/r) e(r/7),                   (32)
and the actual error-covariance matrix VCr/t) is

328                                                 DEMETRIOS G. LAINIOTIS
V(T/t) = P(T/t) [ Fo1 (t. T) + P-1 (T/T) V(,T/r)P-1 (r/r)] P(,T/t),      (33)

where Vo(t, r) and //ie now nominal error-covariance matrix P(r/t) are as de-
fined in Theorem l.

Proof. The proof follows readily from Eqs. (3-4) and is given m reference
[37}.

Remark. From Eqs. (32-33), it is apparent that the estimate is unbiased if:

(a) the initial bias is zero, namely ifx(T/r) = Jc(T/r), and V(r/T) = P(r/T), as in
the Kalman-Bucy filtering theory; or (b) if the initial uncertainty is infinite,
namely ifP(r/T) = "a.

The least-squares, unbiased estimate expressions are obtained from the parti-
tioned smoothing expressions, Eqs. (3-4) simply by putting P(T/r) = °0. They
are given in the following corollary of Theorem 1.

COROLLARY 6.

x(r/t)=Vo(t,T)Mo(.t,T),                     (34)

P(Tlt)=Vo(t,T).                                  (35)

Remarks:

(i) From Eqs. (34-35), it is apparent that Vo(t, r) is the Fisher information
matrix [41]. An important application of it is in the well known Cramer-Rao
(C-R) bound or information inequality. The C-R bound states that ifx(t) is any
unbiased estimate of x(t), then the error-covariance associated withx (t) is
bounded from below by the information matrix. This inequality is valid, of
course, only if Vo1 (t, r) is positive definite. This constitutes the stochastic
observability condition [41-43].

(ii) Further, we note that the least-squares, unbiased estimate, Eqs. (34-35),
is the conditional maximum likelihood estimate, which attains the C-R bound;

hence, it is the efficient, unbiased estimate. However, it is not the minimum
variance estimate as will be seen in the section on error-bounds where the con-
flicting requirements arising from the conditions for unbiasedness and minimum
variance will be discussed.

(iii) For the noise-free plant case, Kalman [43] and Stepner [44] have
obtained related results. Specifically, they have obtained results corresponding
to those of Corollaries 5 and 6, but for the noise-free plant case only.

5. RICCATI EQUATION: PARTITIONED SOLUTIONS

The Riccati equation plays a fundamental role in many fields of science and
engineering, such as estimation [II], control [II], detection [45], and radiative
transfer [46]. Its solution constitutes an integral prerequisite to the solution of
many important engineering problems.

PARTITIONED ESTIMATION ALGORITHMS II                             329

In this section, several new results on the solution of Riccati equations are
presented. These results are obtained as a by-product of the partitioned estima-
tion algorithms, and, to the author's best knowledge, they are new. They are
given in the following Corollaries of Theorems 1 and 3.

COROLLARY 7. The solution of the following Riccati equation arising in
linear estimation:

dp^t)- F(t)P(tlt) + P(tlt)F(t) + 0(0 - Pd^^^R-1 (?) H(t) P(tlt\ (36)

with initial condition P(r/r), t &gt; T, is given by

P(tlt)=P,(tlt) + ^(t,r) [V,\t,7) + P-^r/r)] -1 &lt;(f,T),     (37)
where Py (), ^o (')&gt; and Vo(') are as defined in Theorem 1.

Proof. The corollary follows by direct comparison of the "partitioned" fil-
ter, as given in Theorem 2, to the Kalman filter.

Remark.

(i) We note that the solution of the nonlinear Riccati Eq. (36) with initial
condition P(r/T) is given as the sum of (a) the solution Po (tit) of the same Ric-
cati equation with zero initial condition; and (b) a nonlinear functional of the
initial condition P(rlT). This constitutes a form of a nonlinear superposition
property for the Riccati equation. It is a case of the so-called "linear inverse"
solution of the Riccati equation, Prussing [47].

(ii) As noted previously, the "partitioned" solution of the Riccati equation
serves as the basis of an effective numerical integration algorithm for the solu-
tion of time-varying, continuous Riccati equations. This consists of partitioning
or decomposing the total integration interval into several subintervals, and using
the "partitioned" solution in each subinterval, thus effectively partitioning the
solution of the Riccati equation over the total interval into the decoupled (an-
chored at zero at the initial point of each subinterval) solution of the Riccati
equation over each subinterval. This numerical integration algorithm results in
smaller errors and alleviates the difficulties associated with the numerical inte-
gration of ill-conditioned, time-varying Riccati equations.

The above numerical advantages of the "partitioned" solutions result essen-
tially from the following factors: (a) by "partitioning" the total interval into
small subintervals and anchoring the initial value of each subinterval at zero
drastically reduces the error introduced in the numerical integration by the non-
linear term, by making the linear terms in the Riccati equation the dominant
terms. Namely, for sufficiently small subintervals and starting with zero in each
subinterval, we have II^PII » \\PHTR~l H P\\, since under the above condi-
tions \\P\\ &lt; 1 ; moreover, (b) by partitioning the total interval and decoupling

330                                          DEMETRIOS G. LAINIOTIS

the solutions over each subinterval we avoid the pronounced propagation of the
errors from one interval to the next. The above considerations are dealt with
more thoroughly, the statements are made more precise, and proven conclusively
in reference [50].

In the general, linear stochastic control problem, it is desired to control the
system given by

^=F(Ox(r)+G(?)WW+M(0,             (38)

Z(t)=H(t)x(t)+v(t),                 (39)

where G(t) is the input matrix, W(t) is the desired control, and all other quanti-
ties are as defined in the introduction. The control W(t) must be optimal in the
sense of minimizing the performance index

f         r'

J(t, T) = E\ x^t) Cx(t) +    [xT(a)A(a)x(a)

l                                "T

+WT(a)ß(a)W(a)] da |. (40)

The desired optimal control is given by the linear "separation" theorem [II],
as

W(r) = - B-^T) G^T^T.r) $(r/r),             (41)

where $(T/T) is the filtered estimate given by Eqs. (10-11) or the corresponding
Kalman filter equations, and &gt;S'(r, t) is given by the backward Riccati equation

ds^t} = -F^r) S(r, t) - S(T, t) F(r) - A(r),

+ S(T, t) G(r) B-1 (r) G^T» 5(r, r),            (42)

with final condition S(t, t) = C, ty &lt; T &lt; t.

There exist several methods for solving the backward Riccati equation,
Kleinman [48]. However, there are disadvantages associated with each one,
such as computational instability when the Riccati equation is integrated in the
forward direction, and large storage requirements associated with the offline
solution of the Riccati equation in the backward direction. Several suboptimal
methods have been proposed [49] to alleviate the inability of forward integra-
tion of the Riccati equation.

In the following, the solution of the backward Riccati equation is given ex-
actly in terms of the solution of a related forward Riccati equation.

COROLLARY 8. The solution ofEq. (42) with S(t, t) = 0, denoted So(r, t) is
given by

d-so^t)-^^t,r)A(t)^(t,r),                (43.1)

PARTITIONED ESTIMATION ALGORITHMS II                              331

or

So(r,t)= f &lt;(G, r) A(a)&lt;S&gt;,(o, r) d            (43.2)

-'T

where

/ d) (n T&gt;

.     = [^(") - ^(a, r) A(o)] ^(a, r); &lt;i&gt;,(r, r) = /,       (44)
da

and

dp'Aa^=F(o)Po(a,T)^P,(,o,r)FT(a,T)+G(o)B-l(o)GT(a)
do

-Pc,(a,T)A(a)Po(o,T), (45)
with initial condition P(,(T, r) = 0, r &lt; a &lt; ?.

/^-oo/. The proof consists simply of comparing Eq. (42) to Eq. (31) and
noting that they are identical if we identify Pi,1 (r) as 5o(r, t), Q(r} as G(r)
5-1 (r) G T(T'), and ^(r) A -1 (r) //(r) with ^l(r). Moreover, we note that^1 (r)
was given by the integral expression Eq. (6), hence Eqs. (43.2-45) follow
directly. This proof was also given m [37].

Remarks.

(i) We note that the solution of the backward Riccati equation has been
"partitioned" into the solution of the related "forward" Riccati Eq. (45), and
the system of Eqs. (43-44). Moreover, S'o(T, 0 constitutes the stochastic ob-
servability matrix for a related linear estimation problem.

(ii) We note a significant advantage of Eqs. (43), namely regardless of how
inaccurately $(;() has been computed, &lt;Ï&gt;J'A$(, is always nonnegative definite.
Numerical errors may destroy this property in the quadrature, Eq. (43.2), but
it is much easier to maintain accuracy in a simple quadrature than in the numeri-
cal solution of the Riccati equation (42).

(iii) If we choose the control-cost weighting matrices such that A(a) = HT(a)
R ~1 (a) H(o), and Q(a} = G(a) B~1 (o) G'(a), for r &lt; o &lt; t, then Eqs. (9) and
(45) are identical. Namely, the Riccati equations for the filter and the control
gain, respectively, are identical. Moreover, in this case Vo1 (t, r) = 5'o(r, t).

(iv) It is interesting to consider the time-invariant case, namely when F, G, B,
A are constant. For this case, Kailath [12] has shown that/'o^T) is given by:

d-p^) = ^(t, r) GB-1 G ^(T, t).                (46)
Thus, the time-invariant version of corollary 8 is

COROLLARY 8.

dso^t}=^(t^A 4&gt;,(r,r); So(r,T)=0.
at

(47)

332                                             DEMETRIOS G. LAINIOTIS

d-po(t2I}=^(t,r)GB-ÏGT^(t,^); Po(r,T)=0,        (48)

where

dt

d-M^)= [F- Po(t,T)A] ^(t,T); &lt;Ï&gt;,(r,r) =/. (49)

The general results for S(t, t) = C ^ O are given in the following corollary
COROLLARY 9.

5(T,0=5o(r,0+ [^(r,OC-1 ^(r, /) + C, (r, t)} -',       (50)
w/îe/'e

Q O-, /) = f V/c(r,o) G(o) 2T1 (a) G^a) ^(r, o) d o           (51)

-T

= ^c(ï, t) [  ^(r, o)G(a)5-1 (^G^o^t, a-)da\ ^(r, t)
L*'T                                 J

and

d-^o) = - ^(T, o) [F(a) - G(o) B-1 (a) G ^o) 5o(r, o)] ;

da

and Sy (r, t) was defined m Corollary 8.

^(T,T)=/ (52)

Proof. The proof is straightforward and consists of direct application of
Corollary 8 and the "linear inverse" Theorem [47]. As such it is omitted.

Remarks.

(i) Corollary 9 provides a forward solution of the general backward Riccati
equation with nonzero final cost. Moreover, the solution is given in the form of
a nonlinear superposition theorem, namely m terms of the solution with zero
final condition, So(r, t), and a nonlinear functional of the final condition C.

(ii) Note that the matrix C(.(T, t) constitutes the controllability matrix of the
system. Namely, if the system is to be controllable, then C&lt;,(r, a) must be non-
singular for all a  (r, t). Thus, the complete solution S(r, t) is given in terms of
an observability matrix So (r, t) and a nonlinear functional of the controllability
matrix Cc(T, t) and the final condition C.

(iii) The solution of the backward Riccati equation given by Corollary 9 is
useful in obtaining efficient numerical integration algorithms. These, however,
will be presented elsewhere [50].

PARTITIONED ESTIMATION ALGORITHMS II                             333

6. ESTIMATION ERROR BOUNDS

It is of interest to obtain estimation error bounds which are independent of
the initial conditions, both for the filtering as well as the smoothing error -
covariance matrices. Such bounds are given in the following corollaries of
Theorems 1 and 2.

COROLLARY 10. The filtering error-covariance matrix bounds are

&lt;M?,T) C{t,r) «Î.T) &lt;W) &lt;Po(tlt) + 4&gt;o(f,r) ^o(f,r) ^(t,T), (53)
where

C(t, T) = ) $o(r, o} Q(a) «r, o) d a,              (54)

'T

and 'î&gt;o('),^o(?/0&gt; and Vo(t,r) were defined previously.

Proof. The proof follows readily from Theorems 1 and 2 and can be found
elsewhere [50].

Remarks. We note, as indicated previously, that the above bounds are inde-
pendent of the initial conditions and are given in terms of the inverse of the ob-
servability matrix Vy(t, r) (Fisher information matrix) and the stochastic con-
trollability matrix C(t,r).

Several useful ordering relationships between the least-squares unbiased
estimate variance, Vo(t,r), and the actual and nominal variances for the
smoothed estimate, ^(r/r) and P(r/r), respectively, may be readily obtained
through use of Eqs. (33-35). They are given in the following corollary.

COROLLARY 11.

P(Tlt)^Vo(t,r),                         (55)
V(Tlt)^Vo(t,r),                         (56)

and under the condition that P(T/r) &gt; V{rJT), namely for P(T/T) - ^(r/r) posi-
tive definite, we have the additional relationship

V(Tlt)^P(Tlt).                          (57)

Proof. The proof was given by Lainiotis [37], and hence it is omitted.

Remarks. We note from Eq. (55) that the nominal covariance matrix is
upper-bounded by the variance of the least-squares unbiased estimate, namely
the Fisher information matrix. Moreover, in the case of unbiased estimate due
to initialization as in the Kalman-Bucy filter, namely forP(r/r) = V(T/T), and
$(T/r) = x(r/T), then the actual error-covariance V(rlt) equals P(r/?), and it is,

334                                         DEMETRIOS G. LAINIOTIS

further, the minimum variance among all unbiased estimates. Note, further, that
for the case of infinite initial uncertainty, namely for P(T/r) = °°, the actual and
nominal covariance matrices reduce to the least-squares covariance matrix.

From Eq. (56) we also observe that the actual err or-covariance is also upper-
bounded by Vo(t,r) as well. Furthermore, the actual covariance is bounded
from above by the nominal covariance matrix under the condition that the as-
sumed initial uncertainty P(T/r) is larger than the actual initial uncertainty
7(T/r). A similar result was first obtained by Nishimura [51].

The above considerations point out the fact that unbiasedness and error-
covariance minimization are different estimation optimality criteria leading to
conflicting requirements. However, as pointed out above, there is a unique ex-
ception to this, namely the Kalman-Bucy estimation theory with initial condi-
tions x(r/T) = x(r/7-), andP(r/r) = V(T/T), leading to estimates which are mini-
mum variance as well as unbiased. Similar conclusions to these were also
obtained by Stepner [44] in his study of the noise-free plant case.

7. PARTITIONED LINEAR ESTIMATION ALGORITHMS: DISCRETE CASE

Results exactly analogous to the continuous case results have also been ob-
tained for the discrete case [52]. The particular discrete linear estimation con-
sidered and the corresponding discrete "partitioned" estimation algorithms are
briefly presented below.

Specifically, the discrete linear estimation problem considered is given by

x(k+ï)=&lt;î&gt;(k+l,k)x(k)+u(k),              (58)

z(k)=H(k)x(k)+u(k),                   (59)

where x(k) and z(k) are n and m-dimensional state and observation processes,
respectively; {u(k)}, and [v(k)} are the independent plant and observation
noise processes. They are zero-mean, white gaussian with covariances Q(k) and
R(k), respectively. The initial state -&lt;'(0) =Xy is gaussian with mean $(0/0), and
covariance P(010). Moreover, XQ is independent of {u(k)}, and {u(k)}, for
k&gt;0.

Given the measurement record X^ = {z(i); i = 1,2,   -, k}, the mean-square-
error-sense optimal estimates ofx(l) and x(k), 0 &lt; / &lt; k, denoted x(l/k) and
x(k/k), respectively, are desired. The discrete "partitioned" estimation algo-
rithms are given in the following two theorems.

THEOREM 3. The smoothed estimate and the corresponding covariance
matrix are given by

x(l/k) = P(l/k) [Mo (k, f) + P-1 (l/î) x(lll)],              (60)
P(l|k)=[Vo-l(k, r)+P-1 (Hl)},                   (61)

where x(l/l) and P(l/l) are the filtering estimate and the filtering covariance, re-
spectively, and Afo(-) and Fo1 () are defined as

PARTITIONED ESTIMATION ALGORITHMS II                             335

Mo(M= £ ^U- l,/)//r()^l(/- l) zo(/),        (62)
/=(+i

fo1 (fc, 0 = f; &lt;( -1,0 ^O) P^ (/ - 1 ) HU) $o ( - l, O,   (63)
/=;+i

w/iCT-e Zo(/) = z(/') - //() $o (//), and j? o (/),  &gt; ^ " gï're" ^ ^e Kalman
filter equations

mil) = MA J - l) ^o(/ - 1// - l) + ^o(/) z(/)          (64)
with initial condition $o(//0 = 0;

^o () = Po Ui! - l ) If^f) P?,1 (// - l ),               (65)

P-z, U l i -1 ) = ^(/) ^o (// -1 ) ^O') + R (),             (66)

$0 (,  - l ) = «ï» (, / - l ) [ / - K» (/) //()]              (67)
andPoUli' l) is given by

PoUli- l)-^^,;- l)^o(/- 1//- l)f(/,/- l)+ß(;)     (68)

A) (//) = ^o (/ - l ) - ^o (/) ^(/) ^o (// - l )            (69)
with initial condition Po(lll) = 0.

Proof. The details of the proof can be found in Lainiotis [53], and as such
the proof is omitted.

The discrete "partitioned" filtering algorithm is given in the following
theorem

THEOREM 4. The filtering estimate S(klk) and the filtering error-covariance
matrix P(klk) are given by

S(klk) = So(klk) + &lt;ï&gt;o(fc,0 J?(W,                 (70)

PWk) =PoWk) + &lt;i&gt;oWOW) *o(fc/0,             (71)
where 0 &lt; / &lt; k, and all other quantities are as defined in Theorem 3.

Proof. The proof is given m Lainiotis [53], and as such the proof is omitted.

Remarks. We note the remarkable similarity between the discrete and con-
tinuous "partitioned" estimation algorithms. Moreover, we note that the same
comments apply to the discrete case as to the continuous case. Specifically, the
remarks (i)-(vi) following Theorem 2 pertain to the discrete case as well. The
"partitioned" nature of the discrete algorithm also is due to the partitioning or
decomposition of the innovations into conditional innovations and residuals.
For further comments on this, see Lainiotis [53].

For completeness, the discrete-case results on unbiased estimation and
initialization of the Kalman filter are given below. Specifically, the bias and ac-

336                                          DEMETRIOS G. LAINIOTIS

tuai covariance matrix resulting from initializing the smoothing algorithms with
nominal instead of the actual initial state-vector mean and variance are given in
the following corollary of Theorem 3.

COROLLARY 12. The bias, defined as e(llk) = E [x(l) - x(l/k)], is given by

ed/^^PW^P^Wed/l),                  (72)
and the actual covariance matrix V(l/k) is

V(llk) = P(l/k) [ V,1 (k, l) + P-1 (///) V(î/l) P-1 (///)] P(llk),      (73)

where Vo(k, 1) and the now nominal covariance P(l/k) are as defined in Theo-
rem 3.

Remarks. The conditions for unbiasedness given in connection with the con-
tinuous case apply here also.

The least-squares unbiased estimate expressions are given in the following
corollary which is the discrete equivalent of Corollary 6.

COROLLARY 13.

x(l/k~)=Vo(k,l)Ma(k,l),                    (74)
P(.l/k)=Vo(k,l).                            (75)

Again, for completeness of the presentation of the "partitioned" approach to
linear estimation, as well as for the sake of establishing even more the complete
analogy between the continuous and discrete case, the "partitioned" solutions to
the discrete Riccati equation and the related estimation error bounds are given
below

COROLLARY 14. The solution of the discrete Riccati equation given by the
following equations

P(flf) = [ I - K(f) H(f)] P(f/, - 1 ),                                 (76)
P(, + 1 /) = $(/ + l, ) [P-1 (// - l ) + H^f) R -1 () H(f)] -1 $ ^ + l, )

+ö(/),  (77)

K(f)=P(//f- \)H\j) [H(f)P(,/f- O^O^O)]-1.           (78)
with initial condition P(l/l), f &gt; l, is given in the "partitioned" form

P(f/J) = Po (///) + $0 (/, /) t Vo1 (/, 0 + P~1 (l/l)] ~1 &lt;(/, 0,      (79)
where Po(-), $o0. a'ul Vo(.-) are as defined in Theorem 3.
Proof. The corollary follows readily from Theorem 4.

Remarks. Again, we note that Eq. (79) is the discrete version of the so-called
"linear inverse" solution of the Riccati equation. It constitutes a nonlinear
superposition property for the discrete Riccati equation.

PARTITIONED ESTIMATION ALGORITHMS II                           337

In the general, discrete linear stochastic control problem, the control of the
following system is desired

x(k + 1) = &lt;S&gt;(k + \,k}x(k} + G(A:) w(k) + u(k),          (80)
z(k)=H(k)x(k)+v(k),                          (81)

where G(k) is the input matrix and w(k) is the desired control. All other
quantities are as defined previously. We desire the control sequence {w(/);  =
0,1,   -,N- 1} optimal in the sense of minimizing the performance index

f              N~'i                                1
J = E \x T(N) Cx (N) + ^ [x T(k) A (k) x (k) + w T(k) B (k) w(k)}\ , (82)

l               k=0                                  J

where C,A(') are nonnegative definite matrices and 5(') is a positive definite
matrice.

The optimal control is given by the discrete "linear separation" Theorem
[II],as

w(k) = {-B-1 (k) G^k) [S-1 (k + 1 ,N)

+G(k)B-\k)GT(k)]-l ^(k+l,k)} x{klk\ (83)

where x(klk) is the filtered estimate given by Eqs. (70-71), and S(k + 1 ,N) is
the solution of the backward Riccati difference equation

S(K,N)=&lt;î&gt;T(k+ ï,k) [^"'(fe+l.AO

+ G(k) B~1 (k) GT(k)] ~1 &lt;î&gt;(k + l ,fe) + A(k) (84)

with S(N,N) = C.

The solution of the backward Riccati Eq. (84) is given in terms of the solu-
tion of a forward Riccati equation in the following corollary.

COROLLARY 15. The solution o f Eq.(&amp;4) with S (N, N) =0, denoted
So(K,N) is given by

So(K,N)= Z &lt;(-!, k)DT(J)[D(J)Po(j,j-l)DT(|)+I}
/=k+l

£»()$(/- l,fe), (85)
where

AU^D^j) D(,),                                 (86)
^(A/- 1)= [I- WDU)] $(,- 1),                    (87)
KcU^PoUJ- l)ü^(/) [ü(/)^o(/,/- Oü^ l]-1. (88)

338                                             DEMETRIOS G. LAINIOTIS

and

Po(f + l, /) = ï&gt;(/ + l, /) [Po1 (/, / - l) + A(/)] -' &lt;î&gt;T( + l, /)

+G()5-l()G^(/), (89)

with initial condition Po (1/0) = 0.

Proof. The proof parallels exactly that for the continuous version of the
corollary; thus it is omitted.

Remarks. The remarks made in connection with the corresponding con-
tinuous corollary apply here also. Moreover, as with the continuous results, we
may readily extend the above corollary to the nonzero final cost case, as well as
we may specialize it to the time-invariant case; however, for reasons of space we
forebear from presenting these results here. They can be found elsewhere [53].

In conclusion, we remark again on the similarity of the discrete to the con-
tinuous results, and on the identical nature of the meaning of the above results.

REFERENCES

1. R. E. Kalman, A new approach to linear filtering and prediction problems, Trans. ASME
J. Basic Eng., Sénés D, 82, 34-35 (March 1960).

2. R. E. Kalman, and R. S. Bucy, New results in linear filtering and prediction theory,
Trans. ASME, J. Basic Eng. Series D, 83,95-107 (December 1961).

3. R. E. Kalman, New methods in Wiener filtering theory, Proc. 1st Symp. on Engineering
Applications of Random Function Theory and Probability (J. L. Bogdanoff and
F. Kozin, Eds.) Wiley, New York, 1963.

4. Y. C. Ho, The method of least squares and optimal filtering theory, The Rand Corp.
Memorandum RM-3329-PR, 1962.

5. Y. C. Ho, On the stochastic approximation method and optimal filtering theory, /.
Math. Anal. Appl.,6,152-154,1963.

6. Y. C. Ho, and R. C. K. Lee, A Bayesian approach to problems in stochastic estimation
and control, IEEE Trans. ofAutom. Control, 9, 333-339,1964.

7. H. E. Rauch, F. Tung, and C. T. Striebel, Max. likelihood estimates of linear dynamic
systems, AIAA }., 3,8,1445-1450, (August 1965).

8. M. Athans, and E. Tse, A direct derivation of the optimal linear filter using the maxi-
mum principle, IEEE Trans. Autom. Con., AC-12, No. 6, 690-698 (December 1967).

9. E. B. Stear, and A. R. Stubberud, Optimal filtering for Gauss-Markov noise, Int. J. of
Control, 8, 123, 1968.

10. T. Kailath, An innovations approach to least-squares estimation, Pt. I: Linear filtering
in additive white noise, IEEE Trans, on Autom. Contr., AC-13,646-655 (December
1968).

11.J. Meditch, Stochastic optimal linear estimation and control, McGraw-Hill, New York,
1969.

12. T. Kailath, Some new algorithms for recursive estimation in constant linear systems,

IEEE Trans, on Inform. Theory, 19, no. 6, 750-760 (Nov. 1973).

12a. Levinson, N., The Wiener rms error in filter design and prediction, /. Math. Phys., 25,
No. 1,261-278 (Jan. 1947).

PARTITIONED ESTIMATION ALGORITHMS II                           339

13. A. E. Bryson, and M. Frazier, Smoothing for linear and nonlinear systems, TDR 63-
119, Aero. Sys. Div. Wright-Patterson Air Force Base, Ohio, February 1963.

14. H. E. Rauch, Solutions to the linear smoothing problem, IEEE Trans, on Autom.
Contr., AC-8, No. 4, 371-372 (October 1963).

15. R. C. K. Lee, Optimal Estimation, Identification and Control, MIT Press, Cambridge,
Mass., 1964.

16. H. Cox, On the estimation of state variables and parameters for noisy dynamic systems,
IEEE Trans, on Automatic Control, AC-9,5-12 (January 1964).

17. D. Q. Mayne, A solution of the smoothing problem for linear dynamic systems, Auto-
matica, 4,73-92 (1966).

18. D. C. Fraser, A new technique for thé optimal smoothing of data, Sc.D. Dissertation,
MIT, January 1967.

19. J. S. Meditch, Orthogonal projection and discrete optimal linear smoothing, SIAMJ.
Contr., 5, 74-89 (1967).

20. J. S. Meditch, On optimal linear smoothing, J. Inform. Cont., 10,598-615 (1967).

21. J. S. Meditch, Optimal fixed-point continuous linear smoothing, Proc. 1967 Joint
Autom. Contr. Conf., 249-257 (1967).

22. A. E. Bryson, and L. J. Henrikson, Estimation Using Sampled-Date Containing Se-
quentially Correlated Noise, Technical Report No. 533, Division of Engineering and Ap-
plied Physics, Harvard University, Cambridge, Mass., 1967.

23. R. K. Mehra, On optimal and suboptimal linear smoothing. Technical Report No. 559,
Division of Engineering and Applied Physics, Harvard University, Cambridge, Mass.,
March 1968.

24. A. E. Baggeroer, Ph.D. Dissertation, Department of Electrical Engineering, MIT, Cam-
bridge, Mass., February 1968.

25. T. Kailath, and P. Frost, An innovations approach to least-squares estimation-Part II:

Linear smoothing in additive white noise, IEEE Trans, on Automatic Control, AC-13,
6, 655-660, (December 1968).

26. H. Kwakernaak, Optimal filtering in linear systems with time delays, IEEE Trans.
Autom. Contr., AC-12,169-173 (1967).

27. L. E. Zachrisson, On optimal smoothing of continuous-time kalman processes. Inform.
Sei. l, 143-172 (1969).

28. T. Nishimura, A new approach to estimation of initial conditions and smoothing prob-
lems, £££' Trans. Aerospace Electr. Systems, AES-5,5, 828-836 (September 1969).

29. W. W. Willman, On the linear smoothing problem, IEEE Trans, on Autom. Control,
AC-14, 116-117 (1969)

30. B. D. 0. Anderson, and S. Chiratrattananon, Smoothing as an improvement on filtering:

a universal bound. Electronics Letters, 7, 18, (1971).

31. B. D. 0. Anderson and S. Chiratrattananon, New linear smoothing formulas, IEEE
Trans. Autom. Contr., AC-17,1,160-161 (February 1972).

32. K. K. Biswas, and A. K. Mahalanabis, An approach to fixed-point smoothing problems,
IEEE Trans, on Aerospace and Electronic Systems, AES-8,5, 676-682 (September
1972).

33. Lainiotis, D. G., Partitioned estimation algorithms, I: nonlinear estimation. Inform.
Sei., 1 (3/4), 203-235 (Fall 1974), this issue.

34. D. G. Lainiotis, Optimal adaptive estimation: structure and parameter adaptation. Part
I: Linear models and continuous data, Proc. 1969 IEEE Symp. Adaptive Processes,
November 1969 (Also published as Technical Report No. 74, Electronics Research Cen-
ter, University of Texas, Austin, September 1969).

35. D. G. Lainiotis, Optimal adaptive estimation: structure and parameter adaptation, IEEE
Trans. Autom. Contr., AC-16,2,160-170 (April 1971).

340                                         DEMETRIOS G. LAINIOTIS

36. D. G. Lainiotis, Optimal nonlinear estimation. Intern. J. Contr., 14,6, 1137-1148
(1971).

37. D. G. Lainiotis, Optimal linear smoothing: continuous data case. Intern. J. Contr., 17,
5,921-930(1973).

38. D. G. Lainiotis, Joint detection, estimation and system identification. Inform. Contr. J.,
19,8, 75-92 (August 1971).

39. D. G. Lainiotis, Adaptive pattern recognition: A state-variable approach. Frontiers of
Pattern Recognition, (S. Watanabe, Ed.) Academic Press, New York (1972).

40. B. K. Kinariwala, Analysis of time varying networks, IRE Intern. Convent. Ree., Pt. 4,
268-276 (1961).

41. A. H. Jazwinski, Stochastic processes and filtering theory. Academie Press, New York
(1970).

42. M, Aoki, Optimization of Stochastic Systems, New York: Academic Press, 1967.

43. R. E. Kalman, Fundamental study of adaptive control systems, Tech. Report No. ASD-
TR-61-27, RIAS, Martin Co., 1962.

44. D. E. Stepner, Consideration of bias in linear estimation. Technical Report No. 6304-5,
Center for Systems Research, Stanford University, 1971.

45. T. Kailath, Fredholm resolvents, Wiener-Hopf equations, and Riccati differential equa-
tions, IEEE Trans. Inform. Theory, IT-15, 665-672 (1969).

46. Chandrasekhar, Radiative Transfer, Dover, New York (1960).

47. J. E. Prussing, A simplified method for solving the matrix Riccati equation. Int. Journal
of Contr., 15, 5, 999-1000 (1972).

48. D. L. Kleinman, Suboptimal design of linear regulator systems subject to computer
storage limitation, Dept. ESL-R-297, MIT, February 1967.

49. D. L. Kleinman, and M. Athans, The design of suboptimal time-varying linear systems,
IEEE Autom. Control Trans., AC-13, 2,150-159 (1968).

50. D. G. Lainiotis, Riccati equations: partitioned solutions (submitted for publication).

51. T. Nishimura, On the a priori Inform. Seq. Estimation Prob., IEEE Trans, on Automatic
Control, AC-11, 2 (April 1966).

52. D. G. Lainiotis, Partitioned linear estimation algorithms: discrete case,Proc. 1974
Princeton Conf. Inform. Sei. Systems (March 1974).

53. D. G. Lainiotis, Partitioned linear estimation algorithms: discrete case (to appear,
1974).

Received January, 1974</dp:raw-text><dp:aggregation-type>Journals</dp:aggregation-type><dp:version-number>S350.1</dp:version-number></dp:document-properties><cja:converted-article version="4.5.2" docsubtype="fla" xml:lang="en" xmlns:cja="http://www.elsevier.com/xml/cja/schema">
		<cja:item-info>
			<cja:jid>INS</cja:jid>
			<cja:aid>74900206</cja:aid>
			<ce:pii xmlns:ce="http://www.elsevier.com/xml/common/schema">0020-0255(74)90020-6</ce:pii>
			<ce:doi xmlns:ce="http://www.elsevier.com/xml/common/schema">10.1016/0020-0255(74)90020-6</ce:doi>
			<ce:copyright type="unknown" year="1974" xmlns:ce="http://www.elsevier.com/xml/common/schema"/>
		</cja:item-info>
		<cja:head>
			<ce:title xmlns:ce="http://www.elsevier.com/xml/common/schema">Partitioned estimation algorithms, II: Linear estimation</ce:title>
			<ce:author-group xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:author>
					<ce:given-name>Demetrios G.</ce:given-name>
					<ce:surname>Lainiotis</ce:surname>
				</ce:author>
				<ce:affiliation>
					<ce:textfn>Department of Electrical Engineering, State University of New York at Buffalo, Buffalo, New York 14214 USA</ce:textfn>
				</ce:affiliation>
			</ce:author-group>
			<ce:date-received day="1" month="1" year="1974" xmlns:ce="http://www.elsevier.com/xml/common/schema"/>
			<ce:abstract class="author" xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:section-title>Abstract</ce:section-title>
				<ce:abstract-sec>
					<ce:simple-para view="all">In a radically new approach to linear estimation, Lainiotis [33, 36–37, 52–53], using the “partition theorem”-an explicit Bayes theorem-obtained fundamentally new linear filtering and smoothing algorithms both for continuous as well as discrete data. The new algorithms are given in explicit, integral expressions of a “partitioned” form, and in terms of decoupled forward filters. The “partitioned” algorithms were shown to be especially advantageous from a computational as well as from an analysis standpoint. They are essentially based on the decomposition of the innovations into partial or conditional innovations and residuals.</ce:simple-para>
					<ce:simple-para view="all">In this paper, the “partitioned” algorithms are shown to be the natural framework in which to study such important concepts as observability, controllability, unbiasedness, and the solution of Riccati equations. Specifically, in this paper, the “partitioned” algorithms are re-examined yielding further insight as well as several significant new results on: 
						<ce:list>
							<ce:list-item>
								<ce:label>1.</ce:label>
								<ce:para view="all">(a) unbiased estimation and filter initialization procedures;</ce:para>
							</ce:list-item>
							<ce:list-item>
								<ce:label>2.</ce:label>
								<ce:para view="all">(b) stochastic observability and stochastic controllability;</ce:para>
							</ce:list-item>
							<ce:list-item>
								<ce:label>3.</ce:label>
								<ce:para view="all">(c) the interconnection between stochastic observability, Fisher information matrix, and the Cramer-Rao bound;</ce:para>
							</ce:list-item>
							<ce:list-item>
								<ce:label>4.</ce:label>
								<ce:para view="all">(d) estimation error-bounds; and most importantly</ce:para>
							</ce:list-item>
							<ce:list-item>
								<ce:label>5.</ce:label>
								<ce:para view="all">(e) computationally effective “partitioned” solutions of time-varying matrix Riccati equations. In fact, all of the above results have been obtained for general, time-varying, lumped, linear systems.</ce:para>
							</ce:list-item>
						</ce:list>
					</ce:simple-para>
					<ce:simple-para view="all">In addition, it is shown that previously established smoothing algorithms, such as the Meditch differential algorithm and the Kailath-Frost total innovation algorithm, are readily obtained from the “partitioned” algorithms. The properties of the “partitioned” algorithms are obtained, thoroughly examined, and compared to those of other algorithms.</ce:simple-para>
				</ce:abstract-sec>
			</ce:abstract>
		</cja:head>
		<cja:tail>
			<ce:bibliography view="all" xmlns:ce="http://www.elsevier.com/xml/common/schema">
				<ce:section-title>References</ce:section-title>
				<ce:bibliography-sec>
					<ce:bib-reference id="BIB1">
						<ce:label>1.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.E.</ce:given-name>
										<ce:surname>Kalman</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>A new approach to linear filtering and prediction problems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Trans. ASME J. Basic Eng.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>82</sb:volume-nr>
									</sb:series>
									<sb:date>March 1960</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>34</sb:first-page>
									<sb:last-page>35</sb:last-page>
								</sb:pages>
							</sb:host>
							<sb:comment>Senes D</sb:comment>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB2">
						<ce:label>2.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.E.</ce:given-name>
										<ce:surname>Kalman</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>R.S.</ce:given-name>
										<ce:surname>Bucy</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>New results in linear filtering and prediction theory</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Trans. ASME, J. Basic Eng.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>83</sb:volume-nr>
									</sb:series>
									<sb:date>December 1961</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>95</sb:first-page>
									<sb:last-page>107</sb:last-page>
								</sb:pages>
							</sb:host>
							<sb:comment>Series D</sb:comment>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB3">
						<ce:label>3.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.E.</ce:given-name>
										<ce:surname>Kalman</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>New methods in Wiener filtering theory</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:editors>
										<sb:editor>
											<ce:given-name>J.L.</ce:given-name>
											<ce:surname>Bogdanoff</ce:surname>
										</sb:editor>
										<sb:editor>
											<ce:given-name>F.</ce:given-name>
											<ce:surname>Kozin</ce:surname>
										</sb:editor>
									</sb:editors>
									<sb:title>
										<sb:maintitle>Proc. 1st Symp. on Engineering Applications of Random Function Theory and Probability</sb:maintitle>
									</sb:title>
									<sb:date>1963</sb:date>
									<sb:publisher>
										<sb:name>Wiley</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB4">
						<ce:label>4.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>Y.C.</ce:given-name>
										<ce:surname>Ho</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>The method of least squares and optimal filtering theory</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>The Rand Corp. Memorandum RM-3329-PR</sb:maintitle>
										</sb:title>
									</sb:series>
									<sb:date>1962</sb:date>
								</sb:issue>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB5">
						<ce:label>5.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>Y.C.</ce:given-name>
										<ce:surname>Ho</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On the stochastic approximation method and optimal filtering theory</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>J. Math. Anal. Appl.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>6</sb:volume-nr>
									</sb:series>
									<sb:date>1963</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>152</sb:first-page>
									<sb:last-page>154</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB6">
						<ce:label>6.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>Y.C.</ce:given-name>
										<ce:surname>Ho</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>R.C.K.</ce:given-name>
										<ce:surname>Lee</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>A Bayesian approach to problems in stochastic estimation and control</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. of Autom. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>9</sb:volume-nr>
									</sb:series>
									<sb:date>1964</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>333</sb:first-page>
									<sb:last-page>339</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB7">
						<ce:label>7.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>H.E.</ce:given-name>
										<ce:surname>Rauch</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>F.</ce:given-name>
										<ce:surname>Tung</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>C.T.</ce:given-name>
										<ce:surname>Striebel</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Max. likelihood estimates of linear dynamic systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>AIAA J.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>3</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>8</sb:issue-nr>
									<sb:date>August 1965</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>1445</sb:first-page>
									<sb:last-page>1450</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB8">
						<ce:label>8.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>M.</ce:given-name>
										<ce:surname>Athans</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>E.</ce:given-name>
										<ce:surname>Tse</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>A direct derivation of the optimal linear filter using the maximum principle</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Autom. Con.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-12</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>No. 6</sb:issue-nr>
									<sb:date>December 1967</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>690</sb:first-page>
									<sb:last-page>698</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB9">
						<ce:label>9.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>E.B.</ce:given-name>
										<ce:surname>Stear</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>A.R.</ce:given-name>
										<ce:surname>Stubberud</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal filtering for Gauss-Markov noise</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Int. J. of Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>8</sb:volume-nr>
									</sb:series>
									<sb:date>1968</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>123</sb:first-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB10">
						<ce:label>10.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>T.</ce:given-name>
										<ce:surname>Kailath</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>An innovations approach to least-squares estimation, Pt. I: Linear filtering in additive white noise</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Autom. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-13</sb:volume-nr>
									</sb:series>
									<sb:date>December 1968</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>646</sb:first-page>
									<sb:last-page>655</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB11">
						<ce:label>11.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.</ce:given-name>
										<ce:surname>Meditch</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Stochastic optimal linear estimation and control</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:book>
									<sb:date>1969</sb:date>
									<sb:publisher>
										<sb:name>McGraw-Hill</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB12">
						<ce:label>12.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>T.</ce:given-name>
										<ce:surname>Kailath</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Some new algorithms for recursive estimation in constant linear systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Inform. Theory</sb:maintitle>
										</sb:title>
										<sb:volume-nr>19</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>no. 6</sb:issue-nr>
									<sb:date>1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>750</sb:first-page>
									<sb:last-page>760</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB13">
						<ce:label>12a.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>N.</ce:given-name>
										<ce:surname>Levinson</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>The Wiener rms error in filter design and prediction</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>J. Math. Phys.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>25</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>No. 1</sb:issue-nr>
									<sb:date>Jan. 1947 1947</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>261</sb:first-page>
									<sb:last-page>278</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB14">
						<ce:label>13.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>A.E.</ce:given-name>
										<ce:surname>Bryson</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>M.</ce:given-name>
										<ce:surname>Frazier</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Smoothing for linear and nonlinear systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>TDR 63-119</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>February 1963</sb:date>
									<sb:publisher>
										<sb:name>Aero. Sys. Div. Wright-Patterson Air Force Base</sb:name>
										<sb:location>Ohio</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB15">
						<ce:label>14.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>H.E.</ce:given-name>
										<ce:surname>Rauch</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Solutions to the linear smoothing problem</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Autom. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-8</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>No. 4</sb:issue-nr>
									<sb:date>October 1963</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>371</sb:first-page>
									<sb:last-page>372</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB16">
						<ce:label>15.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.C.K.</ce:given-name>
										<ce:surname>Lee</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal Estimation, Identification and Control</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:book>
									<sb:date>1964</sb:date>
									<sb:publisher>
										<sb:name>MIT Press</sb:name>
										<sb:location>Cambridge, Mass</sb:location>
									</sb:publisher>
								</sb:book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB17">
						<ce:label>16.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>H.</ce:given-name>
										<ce:surname>Cox</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On the estimation of state variables and parameters for noisy dynamic systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Automatic Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-9</sb:volume-nr>
									</sb:series>
									<sb:date>January 1964</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>5</sb:first-page>
									<sb:last-page>12</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB18">
						<ce:label>17.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.Q.</ce:given-name>
										<ce:surname>Mayne</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>A solution of the smoothing problem for linear dynamic systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Automatica</sb:maintitle>
										</sb:title>
										<sb:volume-nr>4</sb:volume-nr>
									</sb:series>
									<sb:date>1966</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>73</sb:first-page>
									<sb:last-page>92</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB19">
						<ce:label>18.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.C.</ce:given-name>
										<ce:surname>Fraser</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>A new technique for the optimal smoothing of data</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Sc.D. Dissertation</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>January 1967</sb:date>
									<sb:publisher>
										<sb:name>MIT</sb:name>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB20">
						<ce:label>19.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.S.</ce:given-name>
										<ce:surname>Meditch</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Orthogonal projection and discrete optimal linear smoothing</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>SIAM J. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>5</sb:volume-nr>
									</sb:series>
									<sb:date>1967</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>74</sb:first-page>
									<sb:last-page>89</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB21">
						<ce:label>20.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.S.</ce:given-name>
										<ce:surname>Meditch</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On optimal linear smoothing</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>J. Inform. Cont.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>10</sb:volume-nr>
									</sb:series>
									<sb:date>1967</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>598</sb:first-page>
									<sb:last-page>615</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB22">
						<ce:label>21.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.S.</ce:given-name>
										<ce:surname>Meditch</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal fixed-point continuous linear smoothing</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:title>
										<sb:maintitle>Proc. 1967 Joint Autom. Contr. Conf.</sb:maintitle>
									</sb:title>
									<sb:date>1967</sb:date>
								</sb:edited-book>
								<sb:pages>
									<sb:first-page>249</sb:first-page>
									<sb:last-page>257</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB23">
						<ce:label>22.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>A.E.</ce:given-name>
										<ce:surname>Bryson</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>L.J.</ce:given-name>
										<ce:surname>Henrikson</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Estimation Using Sampled-Date Containing Sequentially Correlated Noise</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Technical Report No. 533</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>1967</sb:date>
									<sb:publisher>
										<sb:name>Division of Engineering and Applied Physics, Harvard University</sb:name>
										<sb:location>Cambridge, Mass</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB24">
						<ce:label>23.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.K.</ce:given-name>
										<ce:surname>Mehra</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On optimal and suboptimal linear smoothing</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Technical Report No. 559</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>March 1968</sb:date>
									<sb:publisher>
										<sb:name>Division of Engineering and Applied Physics, Harvard University</sb:name>
										<sb:location>Cambridge, Mass</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB25">
						<ce:label>24.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>A.E.</ce:given-name>
										<ce:surname>Baggeroer</ce:surname>
									</sb:author>
								</sb:authors>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Ph.D. Dissertation</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>February 1968</sb:date>
									<sb:publisher>
										<sb:name>Department of Electrical Engineering, MIT</sb:name>
										<sb:location>Cambridge, Mass</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB26">
						<ce:label>25.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>T.</ce:given-name>
										<ce:surname>Kailath</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>P.</ce:given-name>
										<ce:surname>Frost</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>An innovations approach to least-squares estimation-Part II: Linear smoothing in additive white noise</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Automatic Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-13</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>6</sb:issue-nr>
									<sb:date>December 1968</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>655</sb:first-page>
									<sb:last-page>660</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB27">
						<ce:label>26.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>H.</ce:given-name>
										<ce:surname>Kwakernaak</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal filtering in linear systems with time delays</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Autom. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-12</sb:volume-nr>
									</sb:series>
									<sb:date>1967</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>169</sb:first-page>
									<sb:last-page>173</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB28">
						<ce:label>27.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>L.E.</ce:given-name>
										<ce:surname>Zachrisson</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On optimal smoothing of continuous-time kalman processes</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Inform. Sci.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>1</sb:volume-nr>
									</sb:series>
									<sb:date>1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>143</sb:first-page>
									<sb:last-page>172</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB29">
						<ce:label>28.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>T.</ce:given-name>
										<ce:surname>Nishimura</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>A new approach to estimation of initial conditions and smoothing problems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Aerospace Electr. Systems</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AES-5</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>5</sb:issue-nr>
									<sb:date>September 1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>828</sb:first-page>
									<sb:last-page>836</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB30">
						<ce:label>29.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>W.W.</ce:given-name>
										<ce:surname>Willman</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On the linear smoothing problem</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Autom. Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-14</sb:volume-nr>
									</sb:series>
									<sb:date>1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>116</sb:first-page>
									<sb:last-page>117</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB31">
						<ce:label>30.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>B.D.O.</ce:given-name>
										<ce:surname>Anderson</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>S.</ce:given-name>
										<ce:surname>Chiratrattananon</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Smoothing as an improvement on filtering: a universal bound</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Electronics Letters</sb:maintitle>
										</sb:title>
										<sb:volume-nr>7</sb:volume-nr>
									</sb:series>
									<sb:date>1971</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>18</sb:first-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB32">
						<ce:label>31.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>B.D.O.</ce:given-name>
										<ce:surname>Anderson</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>S.</ce:given-name>
										<ce:surname>Chiratrattananon</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>New linear smoothing formulas</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Autom. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-17</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>1</sb:issue-nr>
									<sb:date>February 1972</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>160</sb:first-page>
									<sb:last-page>161</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB33">
						<ce:label>32.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>K.K.</ce:given-name>
										<ce:surname>Biswas</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>A.K.</ce:given-name>
										<ce:surname>Mahalanabis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>An approach to fixed-point smoothing problems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Aerospace and Electronic Systems</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AES-8</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>5</sb:issue-nr>
									<sb:date>September 1972</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>676</sb:first-page>
									<sb:last-page>682</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB34">
						<ce:label>33.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Partitioned estimation algorithms, I: nonlinear estimation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Inform. Sci.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>7</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>3/4</sb:issue-nr>
									<sb:date>1974</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>203</sb:first-page>
									<sb:last-page>235</sb:last-page>
								</sb:pages>
							</sb:host>
							<sb:comment>Fall</sb:comment>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB35">
						<ce:label>34.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal adaptive estimation: structure and parameter adaptation, Part I: Linear models and continuous data</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:title>
										<sb:maintitle>Proc. 1969 IEEE Symp. Adaptive Processes</sb:maintitle>
									</sb:title>
									<sb:conference>November 1969</sb:conference>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Technical Report No. 74</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>September 1969</sb:date>
									<sb:publisher>
										<sb:name>Electronics Research Center, University of Texas</sb:name>
										<sb:location>Austin</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
							<sb:comment>Also published as</sb:comment>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB36">
						<ce:label>35.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal adaptive estimation: structure and parameter adaptation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Autom. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-16</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>2</sb:issue-nr>
									<sb:date>April 1971</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>160</sb:first-page>
									<sb:last-page>170</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB37">
						<ce:label>36.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal nonlinear estimation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Intern. J. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>14</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>6</sb:issue-nr>
									<sb:date>1971</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>1137</sb:first-page>
									<sb:last-page>1148</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB38">
						<ce:label>37.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimal linear smoothing: continuous data case</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Intern. J. Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>17</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>5</sb:issue-nr>
									<sb:date>1973</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>921</sb:first-page>
									<sb:last-page>930</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB39">
						<ce:label>38.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Joint detection, estimation and system identification</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Inform. Contr. J.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>19</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>8</sb:issue-nr>
									<sb:date>August 1971</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>75</sb:first-page>
									<sb:last-page>92</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB40">
						<ce:label>39.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Adaptive pattern recognition: A state-variable approach</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:editors>
										<sb:editor>
											<ce:given-name>S.</ce:given-name>
											<ce:surname>Watanabe</ce:surname>
										</sb:editor>
									</sb:editors>
									<sb:title>
										<sb:maintitle>Frontiers of Pattern Recognition</sb:maintitle>
									</sb:title>
									<sb:date>1972</sb:date>
									<sb:publisher>
										<sb:name>Academic Press</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB41">
						<ce:label>40.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>B.K.</ce:given-name>
										<ce:surname>Kinariwala</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Analysis of time varying networks</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IRE Intern. Convent. Rec.</sb:maintitle>
										</sb:title>
									</sb:series>
									<sb:date>1961</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>268</sb:first-page>
									<sb:last-page>276</sb:last-page>
								</sb:pages>
							</sb:host>
							<sb:comment>Pt. 4</sb:comment>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB42">
						<ce:label>41.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>A.H.</ce:given-name>
										<ce:surname>Jazwinski</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Stochastic processes and filtering theory</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:book>
									<sb:date>1970</sb:date>
									<sb:publisher>
										<sb:name>Academic Press</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB43">
						<ce:label>42.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>M.</ce:given-name>
										<ce:surname>Aoki</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Optimization of Stochastic Systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:book>
									<sb:date>1967</sb:date>
									<sb:publisher>
										<sb:name>Academic Press</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB44">
						<ce:label>43.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>R.E.</ce:given-name>
										<ce:surname>Kalman</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Fundamental study of adaptive control systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Tech. Report No. ASD-TR-61-27</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>1962</sb:date>
									<sb:publisher>
										<sb:name>RIAS, Martin Co</sb:name>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB45">
						<ce:label>44.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.E.</ce:given-name>
										<ce:surname>Stepner</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Consideration of bias in linear estimation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Technical Report No. 6304-5</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>1971</sb:date>
									<sb:publisher>
										<sb:name>Center for Systems Research, Stanford University</sb:name>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB46">
						<ce:label>45.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>T.</ce:given-name>
										<ce:surname>Kailath</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Fredholm resolvents, Wiener-Hopf equations and Riccati differential equations</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. Inform. Theory</sb:maintitle>
										</sb:title>
										<sb:volume-nr>IT-15</sb:volume-nr>
									</sb:series>
									<sb:date>1969</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>665</sb:first-page>
									<sb:last-page>672</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB47">
						<ce:label>46.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:surname>Chandrasekhar</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Radiative Transfer</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:book>
									<sb:date>1960</sb:date>
									<sb:publisher>
										<sb:name>Dover</sb:name>
										<sb:location>New York</sb:location>
									</sb:publisher>
								</sb:book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB48">
						<ce:label>47.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>J.E.</ce:given-name>
										<ce:surname>Prussing</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>A simplified method for solving the matrix Riccati equation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>Int. Journal of Contr.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>15</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>5</sb:issue-nr>
									<sb:date>1972</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>999</sb:first-page>
									<sb:last-page>1000</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB49">
						<ce:label>48.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.L.</ce:given-name>
										<ce:surname>Kleinman</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Suboptimal design of linear regulator systems subject to computer storage limitation</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:book-series>
										<sb:series>
											<sb:title>
												<sb:maintitle>Dept. ESL-R-297</sb:maintitle>
											</sb:title>
										</sb:series>
									</sb:book-series>
									<sb:date>February 1967</sb:date>
									<sb:publisher>
										<sb:name>MIT</sb:name>
									</sb:publisher>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB50">
						<ce:label>49.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.L.</ce:given-name>
										<ce:surname>Kleinman</ce:surname>
									</sb:author>
									<sb:author>
										<ce:given-name>M.</ce:given-name>
										<ce:surname>Athans</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>The design of suboptimal time-varying linear systems</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Autom. Control Trians.</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-13</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>2</sb:issue-nr>
									<sb:date>1968</sb:date>
								</sb:issue>
								<sb:pages>
									<sb:first-page>150</sb:first-page>
									<sb:last-page>159</sb:last-page>
								</sb:pages>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB51">
						<ce:label>50.</ce:label>
						<ce:other-ref>
							<ce:textref>D. G. Lainiotis, Riccati equations: partitioned solutions (submitted for publication).</ce:textref>
						</ce:other-ref>
					</ce:bib-reference>
					<ce:bib-reference id="BIB52">
						<ce:label>51.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>T.</ce:given-name>
										<ce:surname>Nishimura</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>On the 
										<ce:italic>a priori Inform Seq. Estimation Prob.</ce:italic>
									</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:issue>
									<sb:series>
										<sb:title>
											<sb:maintitle>IEEE Trans. on Automatic Control</sb:maintitle>
										</sb:title>
										<sb:volume-nr>AC-11</sb:volume-nr>
									</sb:series>
									<sb:issue-nr>2</sb:issue-nr>
									<sb:date>April 1966</sb:date>
								</sb:issue>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB53">
						<ce:label>52.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Partitioned linear estimation algorithms: discrete case</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:edited-book>
									<sb:title>
										<sb:maintitle>Proc. 1974 Princeton Conf. Inform. Sci. Systems</sb:maintitle>
									</sb:title>
									<sb:date>March 1974</sb:date>
								</sb:edited-book>
							</sb:host>
						</sb:reference>
					</ce:bib-reference>
					<ce:bib-reference id="BIB54">
						<ce:label>53.</ce:label>
						<sb:reference xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema">
							<sb:contribution langtype="en">
								<sb:authors>
									<sb:author>
										<ce:given-name>D.G.</ce:given-name>
										<ce:surname>Lainiotis</ce:surname>
									</sb:author>
								</sb:authors>
								<sb:title>
									<sb:maintitle>Partitioned linear estimation algorithms: discrete case</sb:maintitle>
								</sb:title>
							</sb:contribution>
							<sb:host>
								<sb:book>
									<sb:date>1974</sb:date>
								</sb:book>
							</sb:host>
							<sb:comment>to appear</sb:comment>
						</sb:reference>
					</ce:bib-reference>
				</ce:bibliography-sec>
			</ce:bibliography>
		</cja:tail>
	</cja:converted-article></doc:document>
