# <description> file format 1.0 produced by Consyn2Pp.rb
# run as "./Consyn2Pp.rb"
# on Thu Jun 30 13:47:03 +0800 2011
# format: <format>
TITLE
Learning approaches for developing successful seller strategies in dynamic supply chain management
Abstract

Introduction

A global economy and an increase in customer expectations regarding cost and service have compelled manufacturers to strive to improve their business processes, in particular within their supply chains. A supply chain can be defined as a network of autonomous or semiautonomous business entities collectively responsible for moving a product or service from supplier to customer. It involves such interconnected activities as the procurement of raw materials, competing for customer orders, inventory management, production scheduling, and delivering goods to customers. Supply chain management (SCM) encompasses the management of these activities in a seamless way.

The development of Internet and related information and communication technologies forced companies to review the dynamic nature of SCM and reconsider their supply chain strategies accordingly. New supply chain strategies have been introduced recently, such as Vendor Managed Inventory, Collaborative Planning Forecasting and Replenishment or Efficient Consumer Response [40]. Modern technologies provide great opportunities for improving supply chain strategies at the retail end in particular [40]. Following the trend, this paper is concentrated on the demand part of the SCM, which deals with selling goods to customers. In particular, the paper addresses the issue of choosing the selling strategy that would allow to meet the two main goals of SCM: to “maximize customer value and maximize competitive advantage” [51], p. 24. The paper proposes solutions for finding optimal selling prices, which should be high enough to be profitable and at the same time low enough to be competitive and attractive to customers. The problem can be seen as defined in the first price sealed bid reverse auctions: find the largest price a seller could have offered to a customer and still have won. In our work, we assume that customers place their orders with the retailer who proposes the lowest price. We then solve the task of predicting the lowest prices proposed by all market participants. 

Good prediction helps an organization plan for the future. However, no forecast is 100 percent accurate, and thus forecasting accuracy must be evaluated and multiple methods compared to find the best performing method [51]. In our paper, we investigate the potential of machine learning techniques such as Neural Networks (NN) and genetic programming (GP) to resolve the price forecasting problem. In contrast to traditional statistical and technical analysis methods, learning approaches can react to market irregularities more successfully, and thus, provide more accurate results in the context of dynamic SCM [11]. The paper studies the influence of different parameter settings as well as methods for preprocessing input data on the performance of the learning algorithms. The combination of the best settings has been identified empirically and implemented in a number of price predicting models. The effect of these models on the performance of the whole SCM system has been subsequently studied. The motivation is that expected findings not only could improve a company’s performance while running its supply chains, but can also be applied to predicting various financial instruments and winning bids in online auctions. 

Based on the proposed price predictors, we have developed four different seller strategies when dealing with customers. The first strategy (further referred to as Price Probabilities Predictors) is to predict the probability of the winning price being in a particular interval and bid according to the most probable price. The second strategy (Winning Bid Price Predictor) is to predict the prices based on details of the customer requests for quotes (RFQs), market situation, and results from previous auctions, and bid according to the predicted value. The third strategy (Time-Series Price Predictors) is to predict the lowest and the highest customer order prices for each product based on the time-series of these prices, and bid in between the predicted values. The last strategy (Modeling Competitors’ Behavior) is to model the competitors’ behavior, predict their offer prices, and bid just below them.

Given the complex nature of the SCM domain, it is not obvious which approach would lead to better performance. As the cost of mistakes is prohibitively high, businesses and organizations are reluctant to adopt fully automated systems to deal with supply chain management. Instead, modern textbooks on SCM suggest to use computer simulation models for testing proposed new operating procedures [51]. Computer simulations allow users to try out different strategies or alternatives without actually implementing them in practice. This can reduce costs because changing real systems can be expensive. Simulations can also reduce customer dissatisfaction because customers, in general, do not like to be part of “failed experiments”. Once the value of a new idea has been verified through a carefully constructed simulation, that idea may be implemented with a great deal of confidence. As a general guideline, computer simulation is an appropriate tool for process analysis if [51], (p. 345): • The system has one or more interdependent random variables; • The system dynamics are extremely complex; • The objective is to observe system behaviour over a period of time. 

The system has one or more interdependent random variables;

The system dynamics are extremely complex;

The objective is to observe system behaviour over a period of time.

As SCM environments intrinsically satisfy these properties, they naturally lend themselves to simulation studies. More specifically, following [51], we use a management decision-making game as an effective way to test theories before applying them in practice. 

There have been a number of simulation tools (both research prototypes and commercial software) and computerized trading games developed to analyze supply chain performance [45,50]. The majority of them however propose significantly simplified scenarios compared to real trading environments. In contrast to such simulated environments, the TAC SCM game was introduced by Carnegie Mellon University and the Swedish Institute of Computer Science (SICS) in 2003 as part of the International Trading Agent Competition. 1 http://www.sics.se/tac/page.php?id=13. 1 It has since become an annual event in which a number of teams from around the world compete against each other in a simulated supply chain domain. The competition is now probably the best vehicle for testing SCM agents as it encapsulates many of the tradeoffs that could be found in real SCM environments: time-constraints, network latency, unpredictable opponents, etc. This game has been used to evaluate and compare the performance of the proposed predictive models and seller strategies. 

The rest of the paper is organized as follows. Related work is outlined in Section 2. Section 3 introduces the TAC SCM game scenario. The proposed customer order price predictors are presented in Section 4. Section 5 reports on the results from the experiments comparing the predictors. Section 6 investigates how the predictive models affect the overall SCM system performance. Section 7 closes the paper with the conclusions and future work. 

Related work

There is no unique and universally accepted definition of SCM in the literature [25]. Giannakis et al. provide the list of various definitions of the term that could be found in textbooks from 1982 up to 2001 [25]. Modern textbooks define SCM as “the organization of supply chain activities, including purchasing of raw materials and components from suppliers, distribution of parts and finished goods, and administration of the relationship with customers, in order to maximize customer value and maximize competitive advantage” [51], (p. 24). 

It is not possible to pinpoint when the term SCM first appeared and its original definition. The processes of SCM have been practised by companies since the early 20th century, first discussed in the literature back in the 1950s [20], and the term itself appeared in publications in the 1970s [9]. In the 1980s, the idea of automating SCM business processes became very popular, however, experts treated every entity in the supply chain as a static process isolated from the rest. Many related works in the late 1990s still concentrated only on solving tasks in separate subareas [16,30], omitting a number of key parameters and constraints, such as temporal and capacity constraints [1,2,52]. It is only at the turn of the century when researchers raised the issue of the dynamic nature of the SCM problem domain [50], and started to look at the SCM as an integrated process that faces multiple interrelated constraints [49] and uncertainty [12,39,54]. 

With the advent of the World Wide Web, electronic trading systems have become extremely popular in the last decade. After such basic models as e-Shops and e-Procurement, e-Marketplaces have emerged to integrate all business entities (suppliers, customers, partners, trading agents, competitors, etc.) within one global electronic environment [24]. These newly established economic institutes are dynamic, rapidly growing and changing. Enterprises that wish to remain competitive can no longer rely on static business strategies. They have to be able to cope with the inherently uncertain conditions in these environments. In particular, variable, dynamic pricing is a major characteristic of e-Commerce environments, allowing for prices that change or fluctuate due to different variables, conditions, and situations. Identifying optimal dynamic pricing strategies is vital for companies wishing to succeed when managing their supply chains. 

Many works have appeared recently exploring the problem of dynamic pricing [6,7,18,36,53] which highlights the significance of the issue when participating in e-Commerce. A detailed survey on different price models in electronic business is presented in [41]. A research overview on dynamic pricing can also be found in [19]. Dasgupta et al. [18] look at the problem of dynamic posted pricing for sellers in an environment where they have limited market information. Similar problems are considered in [38], where the authors also introduce their tool for evaluating dynamic agent pricing strategies in a simulated marketplace. 

Financial instruments (prices, stocks, auctions, indices, currency rates, etc.) are often predicted as time-series (i.e. considering only their previous values). The analysis of the existing literature on financial time-series predictions reveals that Neural Networks (NN), Genetic Algorithms (GA), and Genetic Programming (GP) in particular, proved to be the most successful learning techniques in many cases. Fuzzy Logic and Support Vector Machines (SVM) also show promising results. Representative recent studies include: NN [21,33,55]; GA/GP [22,31,46,48]; Fuzzy Logic [11,26,28]; SVM [10]; Hybrid models [15]. 

Ghani [23] proposes several methods for predicting online auction prices using regression, decision trees (C5.0), and NNs. The binary classifier based on NNs demonstrated the highest prediction accuracy 96%. Castillo et al. [11] discuss the application of classical regression models, NNs, Fuzzy Logic, and fractal theory for forecasting time-series of dollar/peso exchange rates, US/Mexico exchange rates and prices of onions and tomatoes in the US market. The researchers conclude that the regression models show the poorest performance, and also that NNs outperform Fuzzy Logic when forecasting in the short-term, while Fuzzy Logic outperforms NNs when forecasting in the long term. Yao et al. [55] report empirical evidence of the applicability of NNs to the prediction of foreign exchange rates and discuss issues on the frequency of sampling, choice of network architecture and forecasting periods, and measures for evaluating the predictive power of a model. Kamruzzamana et al. [33] have developed three NN based forecasting models using standard Back-propagation, Scaled Conjugate Gradient, and Back-propagation with Bayesian Regularization training algorithms for the Australian Foreign Exchange to predict six different currencies against the Australian dollar. They show that NN-based models can closely forecast the Forex market and that NNs outperform the Auto-Regressive Integrated Moving Average (ARIMA) technique. The results reported in [21] suggest that the window size and network architecture do have an important effect on the quality of NN-based time-series forecasts. The paper offers a heuristic for estimating the correct window size using the false nearest neighbor method and singular-value analysis. Zhang et al. [56] have examined the effects of the number of input and hidden nodes in NN as well as the size of the training sample on the in-sample and out-of-sample performance. The authors have found that the number of input nodes has a greater impact on performance than the number of hidden nodes, while a larger number of observations reduce forecast errors. 

A number of studies have also demonstrated the potential of GP [35] for predicting financial time-series [31,48]. Analysis of these studies indicates that the two important issues that affect the predictive ability of GP are (i) the language constructs deployed, and (ii) the pre-processing that is applied to the raw data to render them suitable for learning. On the first issue, previous work indicates that the implementation and inclusion of the correct language constructs can greatly enhance the range and behavioural diversity of possible programs within a programming space [22,48]. On the second issue, the literature suggests that different program representations lend themselves better to different data pre-processing techniques [31]. 

A significant contribution towards investigating the problem of setting optimal prices to offer customers is made by the participants of the TAC SCM game. The methods used by the teams include fuzzy reasoning inference mechanisms [27], additive regression with decision stumps [42], linear regression [3], linear cumulative density function (CDF) [5], reverse CDF [32], continuous knapsack problem [4], dynamic pricing [8,29], and k-nearest neighbors [13,34]. The comparison analysis of various predicting algorithms [43] demonstrates superior performance of machine learning algorithms over traditional statistical approaches. 

This paper further explores the problem of predicting prices in dynamic trading environments and in particular in the TAC SCM.

The TAC SCM scenario

According to the TAC SCM scenario [17], there are six agents competing in the game that act as product manufacturers ( Fig. 1 ). Their main tasks are to buy components from suppliers, assemble 16 different types of personal computers (PCs), and sell them to customers. The behavior of both suppliers and customers are simulated by the TAC server. The aim of each participating manufacturer is to maximize their profit: the agent with the highest bank balance at the end of the game wins. The game lasts for 220 simulated days, 15 seconds of real time each. Each day an agent has to perform the following activities: (i) component procurement, (ii) product sales, (iii) production scheduling, and (iv) delivery scheduling. On the procurement side, the agent sends Request for Quotes (RFQs) for components, and decides which previously sent supplier offers to confirm as orders. On the sales side, the agent gets customer RFQs for PCs, decides on which RFQs to answer with offers and what price to offer. When dealing with customer orders, the agent makes decisions on which PCs to produce and which PCs to deliver to customers. The agent also incurs costs: cost of components, storage costs for keeping an inventory of components and PCs, penalties for late deliveries of customer orders, and for bank overdrafts. Income consists of the revenue from PCs sales and the interest on positive bank balance. 

In the TAC SCM, customers express their desire to buy PCs by sending RFQs to all manufacturers. Every RFQ specifies the PC type, quantity, due date (when the order has to be delivered), reserve price per unit, and penalty that the manufacturer will incur daily during the first 5 days following the due date for late deliveries. If a manufacturer fails to deliver the order within 5 days, then the order is canceled and no further penalty is applied. Having a bundle of customer RFQs, the manufacturers decide on the same day for which RFQs to send offers. Manufacturers can only bid on the offer price for each RFQ which indicates the price for which they would be willing to sell. The offer price should not exceed the reserve price specified in the RFQ. The manufacturers receive customer orders on the next day, if their offer price is accepted by the customers. A customer chooses the lowest price proposed by all manufacturers and places an order. If more than one manufacturers offer the same price, then the winner of the order is chosen randomly. Agents cannot see the prices offered by their opponents. The only source of information about the current market state is the periodic reports. Every day manufacturers receive price reports, in which the lowest and the highest order prices for each PC type are specified, but no information is provided on the agents that offered those prices. In addition, market reports are sent every 20 days, stating the requested volume, order volume, and average order price for each PC type during the reporting period.

Models for predicting customer order prices

Neural Networks (NN) and Genetic Programming (GP) learning techniques are chosen to perform forecasts of customer order prices. According to the literature review [11,23,48,55] these techniques provide better predictions compared to others. 

Among other NNs, the multi-layer perceptron (MLP) with the sigmoid activation function and back-propagation training algorithm is used in this study [37]. To satisfy the codomain of the activation function, inputs are normalized according to formula (1) to be in the interval [0.1; 0.9], using the minimum ( x min ) and maximum ( x max ) allowed values for each input: (1) x i = x i - x min x max - x min · 0.8 + 0.1 . In the case of GP, a panmictic generational genetic algorithm combined with elitism (0.5%) [35] is applied. For initialising the population, ramped-half-and-half tree creation [35] with a maximum depth of 7 is applied to perform a random sampling of the program space. During the run, expression trees are allowed to grow up to depth of 17. The program uses tournament selection with a tournament size of 4, and a mixture of mutation-based variation operators [14]: all nodes mutation, macro mutation, point mutation, swap mutation, grow mutation, truncation mutation, and Gaussian mutation. 

The influence of different parameter settings as well as methods for preprocessing input data on the performance of both learning algorithms (MLP and GP) when predicting customer order prices in the context of SCM are investigated in this study. Alternative settings are deployed in a number of predictive models as described below. The models are tested in the TAC SCM game and compared in terms of accuracy of prediction, complexity of implementation, and computational time. The following highly competitive agents are chosen to be the opponents: TacTex 2007 (the winner of the 2006 and the second agent in the 2007, 2008, and 2009 competitions) [42], PhantAgent 2006 (the winner of the 2007 competition) [47], Maxon 2006 (came 4th in the 2006 and 2007 finals), SouthamptonSCM 2006 (came 3rd in the 2006 second finals) [27], and CrocodileAgent 2005 (came 1st in the 2007 second finals, and became finalist of the 2008 and 2009 competitions) [44]. The agents’ binary code is publicly available. 2 http://www.sics.se/tac/. 2 

As the purpose of machine learning techniques is to evolve system behavior based on empirical data available, we considered all the variables determined in the TAC SCM game specification when building input data sets for the proposed predictive models. We tested various combinations of input data; the parameter values used in this study (and as described below) are set experimentally to provide the best performance of the proposed models in the TAC SCM game and to satisfy its rules. These values may vary should the context change. However, the four seller strategies suggested in the paper are not tied to the game rules and can be used in other competitive trading environments that operate with different information.

To estimate the expected level of fitness of the predictive models and to overcome the overfitting problem, cross-validation is performed using the repeated random sub-sampling validation method [37] in case of both MLP and GP. By observing error dynamics over the validation set, the stopping criteria are set empirically for each model individually: the number of generations in case of GP methods, and number of weight-tuning iterations in case of MLP. 

Price probabilities predictors

The idea in the Price Probabilities Predictors is to identify the interval of prices within which there is the highest probability that the price will fall. A set of ensembles of multi-layer perceptrons (MLP), one set for each product type, is designed to predict order price probabilities ( Fig. 2 ). The possible price range is split into small intervals. Each MLP in the ensemble is assigned to one such interval and predicts the probability of the order price being in this interval. The final price is set to a random value from the interval with the greatest probability (the random element makes prices hard to predict by opponents). Different number of intervals have been tested (see Section 5). There are two strategies for setting the upper limit of the possible price range. In the first one, “Probability Fixed” (PF), the upper price limit is fixed according to the highest price observed in all previously played games. In the second one, “Probability Varied” (PV), the upper limit is set for each RFQ individually according to the customer reserve price (the highest price the customer is willing to pay). The inputs for both models include RFQ details and current market information, such as: type of product requested, its quantity, current date, due date, penalty, customer reserve prices, the lowest and the highest customer order prices for the last three days, and the order level as calculated for the previous day (ratio of the number of orders received from customers to the number of offers sent to them). Along with these attributes, an offered price and the corresponding boolean variable showing if the offer with this price resulted in a customer order are recorded during the games for each RFQ. These records are used for training the models. 

Winning bid price predictor

In the Winning Bid Price Predictor (WP), the idea is to predict customer order prices for each RFQ using RFQ details, current market information and the results from previous auctions. Using this information, the MLP predicts the expected value of the order price. The inputs for the model include: product type, its quantity, current date, due date, penalty, reserve prices (the highest price the customer is willing to pay), the lowest and the highest customer order prices for the last three days, and the current demand level (ratio of the number of RFQs received from the customers to the maximum possible number according to the game specification). Records in the training set map these attributes to the actual order price. The number of hidden units is set to 5 and the learning rate is tuned during the training process according to the dynamics of the prediction error.

Time-series price predictors

In the TAC SCM game, information on the lowest and highest customer order prices for each product type is available from the previous day. In a highly competitive market, the difference between these prices tends to be very low. It has been experimentally established that setting offer prices in between these prices is a relatively competitive strategy. According to this, GP and NN learning techniques are applied to perform time-series forecasts of the lowest and highest customer order prices.

The task in time-series prediction is to obtain a model that provides the best possible approximation of the stochastic system that could have generated an observed time-series. The delayed vectors v are used to derive a model f that maps the vector v to the value x t+1 [31]: (2) x t + 1 = f ( v ) = f x t - ( m - 1 ) τ , x t - ( m - 2 ) τ , … , x t , where m is the embedding dimension (lag window) and τ is the delay time. The embedding dimension essentially defines on which historical data in the series the current value depends on. For this study, τ = 1 and m varies across the models proposed for this group. The goal is to explore how the latter parameter as well as data transformation methods, data normalization methods, and number of days in the future for which prices are predicted influence the accuracy of prediction. GP-based models are described first, followed by NN models. The choices of transformation and normalization methods for preparing input data have been made according to the literature review and preliminary tests separately for NN and GP models in order to achieve better performance for each of the two algorithms independently. 

GP-based time-series predictors

We are interested in exploring GP as an alternative approach to NNs for predicting financial time-series. One of the significant issues in the application of GP as a time-series predicting approach is that of transforming the raw observables into data conducive to learning. In particular, we are interested in applying various data transformation techniques on observables and exploring the impact of these transformations on the GP’s performance. In a financial market setting, one is interested in two types of predictions: short term and long term. Long term predictions are particularly important as they may provide input to the strategic direction of the company and making long term decisions. Single-Step Prediction (SSP) is used to predict one value x t+1 in the time-series. To predict further into the future, Iterated Single Step (ISSP) is used which involves using the predicted outputs as further inputs for the next prediction in the process: x t + 1 ′ = f ( x t - m , … , x t - 1 , x t ) ; m &lt; t , x t + 2 ′ = f ( x t - m + 1 , … , x t , x t + 1 ′ ) ; m &lt; t … x t + k ′ = f ( x t - m + k - 1 , … , x t + k - 2 ′ , x t + k - 1 ′ ) ; m &lt; t , k ⩾ 1 , where k is the prediction step which here is set to 10 to allow us to meet the time scale of the TAC SCM tournament (where one game lasts only 220 simulated days). Inherently, as ISSP makes use of predicted values as opposed to observable ones, this has a significant impact on the accuracy of further predictions: inaccuracies in the predictions are further propagated in the future which results in fitness decrease. 

The embedding dimension m is set to 20 for all GP models to resolve a similar trade-off. On the one hand, the lag window should be narrow enough to allow for collection of sufficient number of representative training cases within 220 simulated days. On the other, the lag window should be wide enough in order to investigate the question of how many previous data points significantly contribute to the predictor’s performance. This is possible due to the hierarchical nature of a tree data structure used in GP. It is generally true that, in the absence of side-effecting primitives, tree-nodes near the root contribute to a greater degree than tree-nodes in lower tree-levels. This feature of GP also allows us to capture information about the effectiveness of specific statistical primitives within the current evaluation context, which in turn provides an indication of their overall appropriateness for the prediction task. 

In this paper, we consider and apply three transformation techniques that have been shown in the literature to work better for GP [31]: integral, differential, and rational transformation. Before the transformation technique is applied, it is vital to have the series normalised to moderate effects of the magnitude of the values. For normalisation purposes, we simply divide each value with the maximum value in the series to convert it in the range (0, 1]. In general, stationary series are preferred to non-stationary ones. A time-series is stationary if its statistical properties such as mean, variance, correlation, etc. are constant over time. This characteristic of stationary time-series makes them easier to predict and most statistical forecasting methods assume stationarity. Although most financial time-series are non-stationary, they can be rendered stationary through the use of appropriate transformation techniques. The statistical techniques can be subsequently applied and the predictions can then be untransformed by reversing the mathematical transformation that was initially used. Only differential and rational transformation render the series stationary, the integral transformation does not. 

 Integral Transformation. This technique works by replacing each value with its moving average which is computed with a smoothing period l according to: (3) x i = 1 l ∑ k = t - l - 1 2 t + l - 1 2 x k , where l is the number of neighbours in the moving average window and x k is the k th element of the delayed vector. A predicted value P (derived through Eq. (3)) can be untransformed by applying: (4) Rev i = 1 l - l 2 P · l · ∑ k l - l 2 x k . Differential transformation This technique is the most commonly used to transform a non-stationary series to a stationary one. Differentiation involves substituting each value by its variance from the mean of its neighbouring data within a predefined interval l: (5) x d = x t - 1 l ∑ i = t - l t - 1 x i , where x d is the difference of x t minus the average of the time-series fragment that includes l previous neighbours. Using x d instead of ( y t ← x t − x t−1 ) makes the series smoother, hence relaxing the requirement to closely fit the trend. A predicted value P (derived through Eq. (5)) can be untransformed by applying: (6) Rev d = P + 1 l ∑ k = l l x k . Rational transformation This technique has the effect of smoothing large magnitudes in the differences of consecutive values therefore resulting in time-series with smaller oscillations. The transformation takes the form: (7) x r = ln x k x k - 1 , where x k and x k−1 are consecutive values in the time-series. A predicted value P (derived through Eq. (7)) can be untransformed by applying: (8) Rev r = e [ P + ln ( x 1 ) ] , where x 1 is the first value of the delayed vector. 

Language constructs

The population size is set to 500 individuals. The individuals employ an expression-tree representation. The primitive language includes constructs for performing a number of arithmetic, mathematical, and statistical operations on the input series, namely: {+, − , *, /, exp, ln, sqrt, sin, cos, mean, std. dev., min, max, skewness, kurtosis, diff, mpd1, mpd2}. While such operations as {+, − , *, /, exp, ln, sqrt, sin, cos, mean, std. dev., min, max} are widely used in GP [35], we use additional statistical operations, which are not common for GP applications. A brief description of these operations follows. 

 Skewness (or the third standardized moment) measures the degree of asymmetry of a probability distribution of data values in a time-series: (9) skew = 1 n - 1 ∑ i = 1 n ( x i - x ¯ ) 3 σ 3 , where x i is a data point, x ¯ is the mean, σ is the standard deviation, and n is the number of data points in the time-series. Positive skewness is observed when the long tail is on the side of the high values of x, while negative skewness appears when the long tail is on the side of the low values of x. A symmetric distribution has a skewness of zero. 

 Kurtosis (or the fourth standardized moment) measures the degree of “peakedness” in a distribution: (10) kurt = 1 n - 1 ∑ i = 1 n ( x i - x ¯ ) 4 σ 4 . Higher kurtosis occurs when more of the variance is due to infrequent extreme deviations, while lower kurtosis refers to frequent modestly-sized deviations. 

 Difference measures the difference between the average values of two halves of a time-series fragment: (11) diff = 1 e - s 2 ∑ i = s e - s 2 + s x i - 1 e - s 2 ∑ i = e - s 2 + s s x i , where s is the start index and e is the end index in the time-series fragment. 

 Pdm1 (or the first order moment) and Pdm2 (or the second order moment) both measure how high-valued data points in a time-series fragment are distributed away from its center. Pdm1 is the average of the time-series values weighted by their absolute distance from the middle point of the time-series fragment: (12) pdm 1 = 1 e - s ∑ i = s s + ( e - s ) x i · i - e - s 2 + s . Pdm2 is the variance of these values: (13) pdm 2 = 1 e - s ∑ i = s s + ( e - s ) x i · i - e - s 2 + s - pdm 1 2 . 

Performance measures

We use the Mean Squared Error (MSE) to evaluate the evolved single step predictors: (14) MSE = 1 N ∑ k = 1 N x k actual - x k pred 2 , where N is the number of training cases and x k actual and x k pred are the actual and predicted values respectively. To evaluate the evolved iterative single step predictors an augmented version of the MSE is used which rewards predictions that make more accurate predictions in the future given that predictors in the future are much harder than in the short term. The Augmented Mean Square Error (AMSE) is defined as follows: (15) AMSE = 1 N ∑ k = 1 N ( 1 + P · A ) · x k actual - x k pred 2 , where P is the prediction step and A is the augmentor co-efficient which is set to 0.05. 

NN-based time-series predictors

We developed a set of MLPs which differ in the number of historical data included in the series (defined earlier as embedding dimension m) and we applied a number of transformation and normalization methods over inputs. Table 1 summarizes the differences. Three different data transformation methods, which are broadly used for financial forecasts using NNs, have been tested in the models: (16) Differential x d = x t - x t - 1 , (17) Rational x r = ln x t x t - 1 (18) Statistical x s = x t - x ¯ σ , where x ¯ = 1 N ∑ t = 1 N x t , σ 2 = 1 N - 1 ∑ t = 1 N ( x t - x ¯ ) 2 , where x t and x t−1 are consecutive data values in a series, x ¯ is the mean of the series values and σ 2 is their variance. 

After performing the transformation, data are normalized to be in the interval (0; 1) that is the range of the sigmoid function, which is used as an activation function. Three different formulas for performing data normalization are applied: (19) Linear Varied x i lv = x i - x min x max - x min , (20) Linear Fixed x i lf = x i - x min x max - x min · 0.8 + 0.1 , (21) Non - linear x i nl = 1 1 + e - x i , where x min and x max are the minimum and maximum values for the corresponding type of data transformation, which are set according to the ones observed in the games. 

Experiments with different numbers of hidden units have been run for each model. It has been established that this number does not substantially influence the prediction performance. Thus, this number is set similarly for all the models. In contrast, the learning rate and number of iterations are set individually for each model and PC type according to the dynamics of prediction error during the training process.

Measuring time-series performance

Using different combinations of the settings described above, a number of time-series models have been developed to predict the lowest customer order prices for each PC type: 14 GP models predicting prices for one-day in the future; 11 GP models predicting prices for ten-days in the future; 8 NN models predicting prices for one day in the future.

The models have been tested in the TAC SCM simulated environment in order to compare their performance. GP and NN models have been tested separately. Three sets of games have been played for both GP and NN models. The first set of 15 games has been run to collect data for initial training of the models. In the next 15 games, the predictors have been used and data have been collected for further training of the models. The last set of 15 games has been played to evaluate the models’ performance. To estimate the models’ performance, the normalized average error (NAE) and Hit Percentage (HIT) [31] have been calculated on average for all PC types across all the games played. The NAE is calculated as follows: (22) NAE = 1 N ∑ i = 1 N abs ( x i - x i ′ ) 1 N ∑ i = 1 N ( x i ) , where x i and x i ′ are the actual and predicted values observed in a case; N is the number of cases observed in all games. 

 HIT demonstrates whether a model follows the trend of the observed time-series. Its value is incremented when actual and predicted prices change in the same direction (both increasing or decreasing) compared to the previous actual price. Thus, the higher the value, the more accurately the model follows the trend. HIT is defined as: (23) HIT = 1 N - 1 ∑ i = 2 N 1 Δ x i &gt; 0 ∧ Δ x i ′ &gt; 0 ∨ Δ x i &lt; 0 ∧ Δ x i ′ &lt; 0 , 0 Δ x i &gt; 0 ∧ Δ x i ′ &lt; 0 ∨ Δ x i &lt; 0 ∧ Δ x i ′ &gt; 0 , where N is the number of considered cases; Δ x i = x i − x i−1 and Δ x i ′ = x i ′ - x i - 1 are the differences of two consecutive points. 

The experimental results of testing GP models demonstrate that all 14 models of the one-day predictors and all 11 models of the ten-day predictors achieve similar results respectively. Figs. 3 and 4 illustrate examples of the lowest actual and predicted order prices for one product type for one-day and ten-day models. As can be seen from Fig. 3, all one-day predictors follow the trend. For ten-day predictors ( Fig. 4), the ten days forecast is plotted as predicted on each consequent 10th day. The “ladder”-shape of the lines means that iterated predictors keep the trend for all future days nearly constant. According to paired t-tests, the difference between the two groups of models (one-day and ten-day predictors) is statistically significant with 95% confidence, while the difference between the models within the groups is not. The rational transformation method gives slightly better results in case of one-day predictors, whereas the differential transformation method works better for ten-day predictors. The GP predictors showed the extensive usage of the latest 5 data points of the input time-series (out of 20 allowed by the lag window m). 

The NN models demonstrated better performance compared to the GP models with the considered settings ( Fig. 5 ). Table 2 summarizes the results from both experiments. The table provides the average NAE and HIT values across all product types. Only the best GP models are included in the table. The NAEs of all the models are normally distributed across all product types. The difference between the best GP predictor and NN predictor is statistically significant with 95% confidence according to paired t-tests. The difference between the NN models is not statistically significant with the exception of the APT and DTF models that show the greatest NAE rate among all the NN predictors. The DTT model demonstrates the highest accuracy of prediction (NAE=0.0059, HIT=0.898), followed by the RTFF model (NAE=0.0060, HIT=0.901) and the RTT model (NAE=0.0061, HIT=0.895). Surprisingly, the DTF model appears to be the worst among all the proposed models. This indicates that the differential transformation method is sensitive to the lag window (the number of data points included in the time-series): the more observables considered, the more accurate the prediction produced. In contrast, all three models which are based on the rational transformation method showed relatively similar results, making this transformation method robust to the lag window. 

It is worth noting that the model that has the lowest NAE rate does not necessarily achieve the highest HIT rate as can be seen from Table 2. Had the difference between the models been significant, the final choice of the model would have to be made based on the nature of the task, i.e. what is more important, to predict the exact price values or the trend (the difference between the sequential data points). 

In games with highly competitive prices, the difference between the prices is very low. Thus, one could suggest that the strategy of just following the previous reported prices could be sufficient for making assumptions on the current day’s winning order prices. To check this hypothesis, the difference between the two consecutive data points of actual prices across all games has been calculated (second column in Table 2). It appears that its rate is similar to the rate of relative errors demonstrated by GP predictors, but not by NN models: all the NN models have much lower rate of relative errors than the difference between two consecutive actual prices. This provides evidence for the effectiveness of the proposed NN methods. 

Modeling competitors’ behavior

According to the TAC SCM game specification, customers choose the lowest price among the ones offered by all sellers. Accurately predicting the competitors’ prices for an RFQ would enable one to identify the lowest price which will be offered to a customer. As bidding records of all agents are available in log files after the end of a game, the idea behind modeling the competitors’ behavior strategy is to use these data to elaborate the models of the opponents. Subsequently, using these models, the competitors’ bidding prices can be predicted for every customer RFQ. GP and NN learning techniques are applied for the task. GP-based models are described first, followed by NN models.

GP-based competitors’ models

GP has been applied first to predict the competitors’ bidding prices for every RFQ depending on the RFQ attributes, market prices, and current demand level (total number of the RFQs sent on the day). The evolved GP models are expressed as syntax trees. The following attributes have been considered as terminals in the primitive set of the GP systems (same set for each competitor): PC type, current date, lead time (due date minus current date), quantity, reserve price, penalty, the lowest reported market price, the highest reported market price, and current demand level. The function set includes arithmetic and mathematical operations such as: {+, − , *, /, exp, log, sqrt, sin, cos}. The population size has been set to 2000 individuals. Their fitness has been measured using the Mean Squared Error (MSE) according to formula (14). 

The advantage of this approach is that it provides an indication of which attributes are being used by a competitor when deciding on the price to offer. At the same time, the major weakness of this approach is that the models have to be trained on the historical data of each competitor which might not always be available in real life. In addition, the time needed to evolve a model for each competitor is significant; at least 10 days of real time. Similarly, computing a result when running the models takes up to 90 seconds, depending on the number of RFQs sent on the day and complexity of the trees evolved. This prevents this approach from being used in the TAC SCM game in particular, where all the decisions have to be made within 15 seconds. In order to test the algorithm, the TAC server settings were changed, so that a game day would last 90 seconds.

NN-based competitors’ models

Since using a GP approach takes a long time to predict the competitors’ offer prices, NNs have been tested for the same purpose. A similar MLP architecture has been constructed for each agent: 13 inputs (PC type, current date, lead time (due date minus current date), quantity, reserve price, penalty, the lowest and the highest market price reported during the previous 3 days, and current demand level), 5 hidden units (the number was set experimentally), and 1 output unit (a competitor’s bidding price). In contrast to GP, it takes no more than 1 hour to train a NN-based model. These models are able to produce the results within a second, which makes them ideal for the TAC SCM game.

The advantages of both the aforementioned methods (GP and NN-based predictive models) have been taken into consideration to implement yet another approach for predicting the competitors’ bidding prices. In particular, the trees have been evolved for each competitor using GP to represent the subset of attributes that a competitor is using when setting its offer prices. According to these trees, an individual MLP has been constructed for each competitor: only those attributes represented in the competitor’s tree have been included as inputs in its MLP.

GP vs NN competitor predictors

As previously discussed, NN-based models predict competitor offer prices faster than GP-based models. In addition, the time required for training GP predictors is significantly longer compared to training NNs. Further comparison of the methods in terms of accuracy of their predictions is provided below. 20 games have been played for each of the proposed techniques to collect data for training the models, and then another 20 games to estimate their performance. The NAE (formula (22)) has been calculated using actual and predicted prices for each competitor across all games played as the measurement of the predictors’ accuracy. According to Table 3 , the predictors give different rates of accuracy for different competitors, showing its highest error for the TacTex agent and its lowest error for the Maxon agent. Identical NN and Individual NN predictors demonstrate similar rate of NAE. However, Individual NN predictors give higher winning rate, which is the ratio of offers accepted by customers (i.e. the number of orders received from the customers in relation to the number of offers sent to them), which leads to better performance while playing games with Individual NN predictors compared to the games played with Identical NN. These results indicate that the agent that knows which parameters its competitors are using for making their bidding decisions performs better than the agent that does not have such knowledge. These observations, namely that the models demonstrate different rate of accuracy for different agents and that Individual NN predictors result in better overall performance, suggest the importance of designing models with individual architectures for each competitor. 

Comparison of the order price predictors

When learning from data, it is interesting to investigate which data should be perceived from the environment, how the data should be preprocessed, and in what way the output can be used for making decisions. The variety of parameter settings implemented in the models introduced earlier help to explore these questions when predicting auction prices. The aim of the experiments that follow is to identify which settings provide more accurate predictions. It was shown that the NN learning technique outperforms GP, thus only NN-based models are compared.

Experimental settings

The tested models have been run simultaneously within the proposed system and the prices predicted by them have been recorded for further analysis. To provide a fair evaluation benchmark, none of the predictive models has been used to set customer offer prices. Instead, the prices have been calculated according to the following formula with random element: (24) Offerprice = ( p highest + p lowest ) / 2 + a 1 - a 2 , where p lowest and p highest are the lowest and highest customer order prices reported the previous day; a 1 and a 2 are coefficients set to random values within the interval [0..20] (the upper limit of the interval is set according to the average gap between the lowest and highest customer order prices). Offer prices set close to the order prices from the previous day guarantee that the agent behaves competitively in the game. The introduction of the random element to the formula makes it harder for other competitors to predict the agent’s prices, brings variety into the price distribution, and makes offer prices different from the ones predicted by time-series models. The following models have been tested: • Price probability predictors (4 models, Section 4.1): PF-50: Predictor of price probabilities with fixed price limits and 50 price intervals, the offer price is set to the middle of the most probable price interval; PF-25: same as above but with 25 price intervals; PV-40: predictor of price probabilities with varied price limits and 40 price intervals, the offer price is set to the middle of the most probable price interval; PV-20: same as above but with 20 price intervals; • Winning Price Predictor ( WP): trained on actual values of these prices; the offer price is set as predicted (Section 4.2); • Time series predictor ( TB): the offer price is set in between the lowest and highest order prices predicted by the NN model which performs the best on average (Section 4.3); • Competitors’ price predictors ( CI): NN-based predictors; the offer price is set to the lowest predicted of all competitors; an individual architecture is used for each competitor (Section 4.4). 

Price probability predictors (4 models, Section 4.1): 

 PF-50: Predictor of price probabilities with fixed price limits and 50 price intervals, the offer price is set to the middle of the most probable price interval; 

 PF-25: same as above but with 25 price intervals; 

 PV-40: predictor of price probabilities with varied price limits and 40 price intervals, the offer price is set to the middle of the most probable price interval; 

 PV-20: same as above but with 20 price intervals; 

Winning Price Predictor ( WP): trained on actual values of these prices; the offer price is set as predicted (Section 4.2); 

Time series predictor ( TB): the offer price is set in between the lowest and highest order prices predicted by the NN model which performs the best on average (Section 4.3); 

Competitors’ price predictors ( CI): NN-based predictors; the offer price is set to the lowest predicted of all competitors; an individual architecture is used for each competitor (Section 4.4). 

30 games have been played to collect the data for training the models and then another 40 games to estimate their performance.

Experimental results

The proposed predictive models are compared in terms of their average relative error (NAE) calculated across all the games played according to formula (22). The NAE values along with their standard deviations for all the models are provided in Table 4 . The Table also indicates the percentage of cases when prices have been predicted higher than their actual values. 

According to the experimental results, all the models provide good predictions with the NAE ranging in between 3–12%. The model which performs time-series forecasts shows the highest accuracy of prediction (NAE = 3.2%) followed by the WP model (NAE = 3.5%). However, this difference is not significant according to the non-parametric statistical method one-way ANOVA performed in Matlab. All other models show significantly different results. The models predicting price probabilities (PF and PV) demonstrate the worst performance (NAE = 11%). The number of ensembles (price intervals) in the PF and PV models influences their prediction accuracy: the models with a smaller number of ensembles predict more accurately. Despite a similar NN architecture, the PF and PV models show the opposite rates of higher predicted prices: both PF models predict higher than actual price values in 80% of cases, while both PV models predict higher only in 38% of cases. 

As all the models are implemented using the same technique (namely, NN), they can also be compared in terms of their complexity. If the data transformation stage needed for the time-series models is not taken into account, then the time complexity of these models as well as the WP model in terms of the Big O notation is linear, i.e. the running time of the algorithms is directly proportional to the number of inputs. The time-series model uses 11 data points at the most, thus is faster than the WP model which uses 13 inputs. However, if the additional computation required for preparing time-series inputs as well as the number of products is taken into account, then the time-series models become more complex by a constant compared to the WP model. In contrast to these two types of methods, the complexity of the probability predictors and competitors’ price predictors is quadratic, as their execution time and memory usage depend not only on the number of inputs, but also on the number of price intervals in case of the probability predictors, and the number of competitors in case of the competitor predictors. 

Finding successful seller strategies in the SCM

In the domain of SCM, it is important not only to predict customer prices accurately, but also to investigate how such a prediction affects the overall performance, i.e. if it allows to achieve higher profit. This section reports on experimental results which aim to (1) investigate if there is a correlation between the accuracy of the models’ predictions and the overall system performance; and (2) identify the most successful strategy for sellers.

The best predictive models from each group are considered: • PV: predictor of price probabilities with varied price limits and 20 price intervals (Price Probabilities Predictor, Section 4.1); • WP: predictor of winning prices trained on actual values of these prices (Winning Bid Price Predictor, Section 4.2); • TB: time-series predictor with the bidding price set to the one predicted by the model which performs the best on average (Time-Series Price Predictors, Section 4.3); • CI: predictor of the lowest price offered by all competitors; individual NN architecture is used for each competitor (Modeling Competitors’ Behavior, Section 4.4). 

 PV: predictor of price probabilities with varied price limits and 20 price intervals (Price Probabilities Predictor, Section 4.1); 

 WP: predictor of winning prices trained on actual values of these prices (Winning Bid Price Predictor, Section 4.2); 

 TB: time-series predictor with the bidding price set to the one predicted by the model which performs the best on average (Time-Series Price Predictors, Section 4.3); 

 CI: predictor of the lowest price offered by all competitors; individual NN architecture is used for each competitor (Modeling Competitors’ Behavior, Section 4.4). 

The models have been tested in pairs: two versions of the system with different predictors have been playing in the same game against each other and the number of the games won by each of them has been calculated. The aim is to identify the most successful strategy among the proposed, rather than to calculate the exact profit margins for each strategy. Therefore, actual scores are not taken into consideration and the results are not compared to the ones achieved by other competitors. The following four TAC SCM agents have been chosen as opponents: PhantAgent 2006, Maxon 2006, SouthamptonSCM 2006, and CrocodileAgent 2005. For each set of paired games, 30 of them have been played to collect the data for training the predictive models and 40 games have been used to estimate the systems’ performance.

According to the experimental results provided in Table 5 , the systems with TB and WP models perform similarly well, outperforming other versions of the system which use CI or PV predictive models. The system with PV model achieves the lowest score. As TB and WP models provide the most accurate forecasts, the conclusion can be drawn that there is a strong correlation between the models’ accuracy of prediction and total score achieved in games. At the same time, as can be observed from Table 5, the models leading to better overall performance do not necessarily provide higher percentage of winning orders (the ratio of the number of offers sent, to the number of orders received). For example, the strategy of predicting competitors’ prices ( CI model), which comes third, provides the highest percentage of winning bids compared to all other strategies. It is worth noting that this result is not due to the model predicting lower prices compared to their actual values. Examining Table 4, the CI model has 60% of higher predicted rate. Hence, it is important to carefully choose a predictor according to the goal set: increase accuracy of prediction, increase overall profit, or number of winning bids. 

Conclusions

This paper investigated the problem of predicting customer order prices in the domain of SCM. In particular, we considered the problem of predicting winning bids in first price sealed bid reverse auctions from the seller’s perspective. The Neural Networks (NN) and genetic programming (GP) learning techniques were applied to make the predictions. Various settings for both algorithms were proposed in an attempt to improve the forecasting results. The settings were deployed in a number of predicting models grouped into four different seller strategies. The experiments in the TAC SCM environment demonstrated that time-series forecasts and price predictions based on RFQ details and bidding history provide the best performance. The systems with these algorithms achieve similar scores when competing against each other. The system with the competitor price predictors came next, and the approach of predicting price probabilities gave the lowest result. The same ranking order is observed when comparing the accuracy of the models’ predictions. Thus, there is a strong correlation between the accuracy of price predictions and the total profit made: the higher the accuracy, the better the overall system performance.

The problem of setting optimal customer offer prices has been explored by other TAC SCM participants. The main contribution of our work is the development, analysis, and comparison of a large number of predictive models that have not been applied yet. Within the scope of chosen parameters, we found that the NN learning technique provided better results in the TAC SCM game compared to GP. The NN-based predictors are more accurate, less complex in implementation and require less time for training and computing the results. At the same time, more research is required in order to improve the predictive abilities of the GP-based models. For the NN-based predictors, it would be interesting to explore training algorithms other than the back-propagation (e.g. the Levenberg-Marquardt approach). The best identified models should be compared with the ones proposed by other researchers, and also in other trading environments and for other financial instruments.

One more important contribution of this paper is that it showed how the customer winning bidding prices can be predicted via modeling the competitors’ behavior. Despite the relatively lower rate of prediction accuracy, the competitors’ price predictors demonstrated promising results. The experiments confirmed that the strategy of modeling and predicting opponents is a suitable technique for environments that operate in a game-like way (i.e. a number of participants compete against each other in order to get the best score): it can help in making more precise decisions in dynamic and competitive settings. Designing individual predictive models for each opponent improves the performance, compared to using the same model for each competitor. Hence, an interesting problem is determining which information and features the competitors are using for making their decisions. In this work, GP has been applied to evolve individual behavioral models of the competitors. The evolved trees revealed which features from the considered set a particular opponent relies on when deciding which prices to offer customers. It would be desirable to explore other techniques to solve this feature selection problem. For example, another genetic algorithm such as Population Based Incremental Learning (PBIL) from the Estimation of Distribution Algorithms (EDA) group may be suitable for the task. The Sequential Search methods, including Backwards Search, Forward Search, and Forward Floating Search are also widely used in feature selection and would be explored.

Acknowledgement

The authors would like to thank Dr Alexandros Agapitos for providing the GP code that was used for the respective experiments presented in the paper.

References

